{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a775c891",
   "metadata": {},
   "source": [
    "# Build a ML pipeline to recommend top 3 games to bet on (within games played that day) for each player each day at ~8AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2615dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from datetime import timedelta\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Set, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import scipy.sparse as sp\n",
    "import s3fs \n",
    "from implicit.bpr import BayesianPersonalizedRanking\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb54cc2b",
   "metadata": {},
   "source": [
    "### Constants ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be838d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separators\n",
    "_RE_AT = re.compile(r\"\\s*@\\s*|\\s*\\bat\\b\\s*\", flags=re.IGNORECASE)     # AWAY @ HOME / AWAY at HOME\n",
    "_RE_VS = re.compile(r\"\\s*\\bvs\\.?\\b\\s*|&\", flags=re.IGNORECASE)         # HOME vs AWAY / HOME & AWAY\n",
    "_SEP = re.compile(r\"\\s*(?:@|&|\\bvs\\.?\\b|\\bat\\b)\\s*\", flags=re.IGNORECASE)\n",
    "# ============ EVAL CONFIG ============\n",
    "BOX_SCORES_XLSX = \"NBA Box Scores.xlsx\"     # local path\n",
    "BOX_SCORES_SHEET = \"Sheet2\"\n",
    "VENDOR_CSV = \"playoff_schedule.csv\"         # local path\n",
    "\n",
    "DATE_START = \"2025-05-01\"\n",
    "DATE_END   = \"2025-05-14\"\n",
    "OUTPUT_CSV = \"./rankings_may01_14.csv\"\n",
    "TOP_K = 3\n",
    "TOP_N_CAL = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4f7b578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_colwidth\", 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2a5742e7-1e1a-4a49-ba37-592724a14ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH1 = Path(r\"df_train.xlsx\")\n",
    "DATA_PATH2 = Path(r\"df_validation.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1bc169",
   "metadata": {},
   "source": [
    "### Workflow begins ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d7bc8773",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_excel(DATA_PATH1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1fe23eec-5ccd-415e-b5c1-bc81ae086354",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = pd.read_excel(DATA_PATH2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a531af00-9f88-41a3-9143-e02aa2994e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32672, 4)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "55662a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows in df_validation: 2429\n"
     ]
    }
   ],
   "source": [
    "num_duplicates = df_validation.duplicated().sum()\n",
    "#print(f\"Number of duplicate rows in df_validation: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa9362-da3d-49e2-ad49-dd73a509439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "playoff_schedule=pd.read_csv('playoff_schedule.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a3a54b79-6e9b-44d8-96b4-f2b42c3ae0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Attach the scheduled game date to each validation row by matching event text.\n",
    "\n",
    "This routine aligns rows in `df_validation` (e.g., historical bets or validations)\n",
    "to the vendor's playoff schedule in `playoff_schedule` by normalizing and matching\n",
    "event descriptions (e.g., \"lakers @ warriors\"). It handles home/away text\n",
    "ambiguity by generating both \"A @ B\" and \"B @ A\" variants from a schedule field\n",
    "formatted like \"Team A & Team B\", then performs a left join on the normalized text.\n",
    "\n",
    "Matching rules and tie-breaking:\n",
    "- Text is normalized case- and space-insensitively (lowercased, trimmed, collapsed spaces).\n",
    "- Only schedule dates on or after the bet's date are considered (bet_date <= Date).\n",
    "- If multiple scheduled dates match a single validation row (e.g., teams meet several times),\n",
    "  the earliest valid schedule date is selected deterministically.\n",
    "  \"\"\"\n",
    "# --- Copies so we don't mutate your originals ---\n",
    "playoff_schedule = playoff_schedule.copy()\n",
    "df_validation = df_validation.copy()\n",
    "\n",
    "# 1) Parse dates\n",
    "playoff_schedule[\"Date\"]   = pd.to_datetime(playoff_schedule[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "df_validation[\"bet_date\"]  = pd.to_datetime(df_validation[\"betdate\"], errors=\"coerce\").dt.date\n",
    "\n",
    "# 2) Build Game1 / Game2 from \"Team A & Team B\"\n",
    "teams = playoff_schedule[\"Game\"].str.split(\" & \", n=1, expand=True)\n",
    "playoff_schedule[\"TeamA\"] = teams[0].str.strip()\n",
    "playoff_schedule[\"TeamB\"] = teams[1].str.strip()\n",
    "playoff_schedule[\"Game1\"] = playoff_schedule[\"TeamA\"] + \" @ \" + playoff_schedule[\"TeamB\"]\n",
    "playoff_schedule[\"Game2\"] = playoff_schedule[\"TeamB\"] + \" @ \" + playoff_schedule[\"TeamA\"]\n",
    "\n",
    "# 3) Long schedule for matching\n",
    "sched_long = playoff_schedule.melt(\n",
    "    id_vars=[\"Date\"],\n",
    "    value_vars=[\"Game1\", \"Game2\"],\n",
    "    value_name=\"event_description\"\n",
    ").drop(columns=[\"variable\"])\n",
    "\n",
    "# 4) Light normalization (case/space-insensitive match)\n",
    "def norm(s: pd.Series) -> pd.Series:\n",
    "    return (s.astype(str)\n",
    "              .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "              .str.strip()\n",
    "              .str.lower())\n",
    "\n",
    "sched_long[\"event_key\"]     = norm(sched_long[\"event_description\"])\n",
    "df_validation[\"event_key\"]  = norm(df_validation[\"event_description\"])\n",
    "\n",
    "# 5) Join on normalized event text\n",
    "tmp = (df_validation\n",
    "       .reset_index()  # keep original row index as column \"index\"\n",
    "       .merge(sched_long[[\"Date\", \"event_key\"]],\n",
    "              on=\"event_key\",\n",
    "              how=\"left\"))\n",
    "\n",
    "# 6) Keep only matches where bet_date <= Date\n",
    "tmp = tmp[tmp[\"bet_date\"] <= tmp[\"Date\"]]\n",
    "\n",
    "# 7) ✅ FIX: pick earliest Date per original row by sorting then deduping\n",
    "#    (This guarantees at most one match per df_validation row.)\n",
    "best = (tmp.sort_values([\"index\", \"Date\"])\n",
    "           .drop_duplicates(subset=\"index\", keep=\"first\")\n",
    "           [[\"index\", \"Date\"]])\n",
    "\n",
    "# 8) Write back to df_validation\n",
    "df_validation[\"Scheduled Game Date\"] = pd.NaT\n",
    "if not best.empty:\n",
    "    df_validation.loc[best[\"index\"].to_numpy(), \"Scheduled Game Date\"] = best[\"Date\"].to_numpy()\n",
    "\n",
    "# 9) (Optional) Clean up helpers\n",
    "playoff_schedule.drop(columns=[\"TeamA\", \"TeamB\"], inplace=True, errors=\"ignore\")\n",
    "df_validation.drop(columns=[\"event_key\"], inplace=True, errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "76b75267-e92f-43a4-95c7-4247386efced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32672, 6)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837fe1b-9a27-4aed-8223-da8d268b90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Box_Score=pd.read_excel(\"NBA Box Scores.xlsx\",sheet_name=\"Sheet2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a82c6282-2656-445c-ac02-e63655b73170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23653/1469618187.py:43: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_validation.loc[best[\"index\"].to_numpy(), \"Offical Game Date\"] = best[\"Game Date\"].to_numpy()\n"
     ]
    }
   ],
   "source": [
    "### Same concept as above but for actual games played (Box Scores)\n",
    "\n",
    "# --- Copies ---\n",
    "Box_Score = Box_Score.copy()\n",
    "df_validation = df_validation.copy()\n",
    "\n",
    "# 1) Parse dates\n",
    "Box_Score[\"Game Date\"]   = pd.to_datetime(Box_Score[\"Game Date\"], errors=\"coerce\")\n",
    "df_validation[\"bet_date\"] = pd.to_datetime(df_validation[\"bet_date\"], errors=\"coerce\")\n",
    "\n",
    "# 2) Build schedule-like table\n",
    "sched = Box_Score.rename(columns={\"Modified Game Name\": \"event_description\"})[[\"Game Date\", \"event_description\"]]\n",
    "\n",
    "# 3) Normalization\n",
    "def norm(s: pd.Series) -> pd.Series:\n",
    "    return (s.astype(str)\n",
    "              .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "              .str.replace(r\"\\.+$\", \"\", regex=True)  # drop trailing periods\n",
    "              .str.strip()\n",
    "              .str.lower())\n",
    "\n",
    "sched[\"event_key\"]         = norm(sched[\"event_description\"])\n",
    "df_validation[\"event_key\"] = norm(df_validation[\"event_description\"])\n",
    "\n",
    "# 4) Left-merge (do NOT filter rows out)\n",
    "tmp = (df_validation\n",
    "       .reset_index()  # preserves original row id as \"index\"\n",
    "       .merge(sched[[\"Game Date\", \"event_key\"]],\n",
    "              on=\"event_key\",\n",
    "              how=\"left\"))\n",
    "\n",
    "# 5) Enforce bet_date <= Game Date without dropping rows:\n",
    "#    mark invalid matches as NaT, then take the earliest valid date per row.\n",
    "valid = tmp[\"Game Date\"].ge(tmp[\"bet_date\"])\n",
    "tmp.loc[~valid, \"Game Date\"] = pd.NaT\n",
    "\n",
    "# 6) Pick the earliest valid date per original row (min skips NaT)\n",
    "best = (tmp.groupby(\"index\", as_index=False, sort=False)[\"Game Date\"]\n",
    "           .min())  # NaT if no valid match\n",
    "\n",
    "# 7) Write back to df_validation\n",
    "df_validation[\"Offical Game Date\"] = pd.NaT\n",
    "df_validation.loc[best[\"index\"].to_numpy(), \"Offical Game Date\"] = best[\"Game Date\"].to_numpy()\n",
    "\n",
    "# 8) Clean up\n",
    "df_validation.drop(columns=[\"event_key\"], inplace=True, errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "869ad8f5-f9a8-4813-81da-5f8b03d40fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32672, 7)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c78fd5e2-9786-424f-aeca-efd987ff6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA :compare preseason schedule to actual games played and quanity mismatches\n",
    "\n",
    "# --------- Inputs (already loaded) ----------\n",
    "# playoff_schedule: columns like [\"Date\", \"Game\"] where Game = \"Team A & Team B\"\n",
    "# Box_Score: columns like [\"Official Game Date\" or \"Game Date\", \"Modified Game Name\" or \"Match Up\"]\n",
    "# -------------------------------------------\n",
    "\n",
    "# 1) Choose date/name columns safely\n",
    "box = Box_Score.copy()\n",
    "sched = playoff_schedule.copy()\n",
    "\n",
    "box_date_col  = \"Official Game Date\" if \"Official Game Date\" in box.columns else \"Game Date\"\n",
    "box_game_col  = \"Modified Game Name\" if \"Modified Game Name\" in box.columns else \"Match Up\"\n",
    "\n",
    "# 2) Parse dates (schedule often day-first; box often month/day)\n",
    "sched[\"Date\"] = pd.to_datetime(sched[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "box[box_date_col] = pd.to_datetime(box[box_date_col], errors=\"coerce\")\n",
    "\n",
    "# 3) Helpers to normalize team names and extract matchup keys (order-invariant)\n",
    "def norm_team(t: str) -> str:\n",
    "    \"\"\"Lowercase, remove extra spaces/punctuation (but keep alphanumerics and spaces).\"\"\"\n",
    "    s = str(t)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)          # collapse whitespace\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9 ]+\", \"\", s)   # drop punctuation\n",
    "    return s\n",
    "\n",
    "def matchup_key_from_ampersand(s: str):\n",
    "    \"\"\"From 'Team A & Team B' -> tuple(sorted(norm(A), norm(B))).\"\"\"\n",
    "    parts = [p.strip() for p in str(s).split(\"&\", 1)]\n",
    "    if len(parts) != 2:\n",
    "        return None\n",
    "    a, b = norm_team(parts[0]), norm_team(parts[1])\n",
    "    return tuple(sorted((a, b)))\n",
    "\n",
    "def matchup_key_from_at(s: str):\n",
    "    \"\"\"From 'Team A @ Team B' -> tuple(sorted(norm(A), norm(B))).\"\"\"\n",
    "    parts = [p.strip() for p in str(s).split(\"@\", 1)]\n",
    "    if len(parts) != 2:\n",
    "        return None\n",
    "    a, b = norm_team(parts[0]), norm_team(parts[1])\n",
    "    return tuple(sorted((a, b)))\n",
    "\n",
    "# 4) Build per-date sets of matchup keys for each source\n",
    "sched_keys = (sched\n",
    "    .assign(_key=sched[\"Game\"].map(matchup_key_from_ampersand))\n",
    "    .dropna(subset=[\"_key\", \"Date\"])\n",
    "    .groupby(sched[\"Date\"].dt.date)[\"_key\"]\n",
    "    .apply(lambda x: set(x))\n",
    "    .rename(\"sched_set\"))\n",
    "\n",
    "box_keys = (box\n",
    "    .assign(_key=box[box_game_col].map(matchup_key_from_at))\n",
    "    .dropna(subset=[\"_key\", box_date_col])\n",
    "    .groupby(box[box_date_col].dt.date)[\"_key\"]\n",
    "    .apply(lambda x: set(x))\n",
    "    .rename(\"box_set\"))\n",
    "\n",
    "# 5) Outer join on date to compare both sides\n",
    "cmp = (pd.concat([sched_keys, box_keys], axis=1)\n",
    "         .reset_index()\n",
    "         .rename(columns={\"index\": \"Date\"}))\n",
    "\n",
    "# 6) Compute per-date differences\n",
    "def set_len(x): return len(x) if isinstance(x, set) else 0\n",
    "def set_diff(a, b):\n",
    "    a = a if isinstance(a, set) else set()\n",
    "    b = b if isinstance(b, set) else set()\n",
    "    only_a = a - b\n",
    "    only_b = b - a\n",
    "    return pd.Series({\n",
    "        \"schedule_count\": len(a),\n",
    "        \"official_count\": len(b),\n",
    "        \"only_in_schedule\": sorted(list(only_a)),\n",
    "        \"only_in_official\": sorted(list(only_b)),\n",
    "        \"mismatch_count\": len(only_a) + len(only_b),\n",
    "    })\n",
    "\n",
    "summary = (cmp\n",
    "    .apply(lambda row: set_diff(row.get(\"sched_set\"), row.get(\"box_set\")), axis=1)\n",
    "    .join(cmp[[\"Date\"]]))\n",
    "\n",
    "# 7) Expand long-form mismatches (optional, handy for investigation)\n",
    "def explode_mismatches(df):\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        d = r[\"Date\"]\n",
    "        for mk in r[\"only_in_schedule\"]:\n",
    "            rows.append({\"Date\": d, \"source\": \"schedule_only\", \"matchup_key\": mk})\n",
    "        for mk in r[\"only_in_official\"]:\n",
    "            rows.append({\"Date\": d, \"source\": \"official_only\", \"matchup_key\": mk})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "mismatches_long = explode_mismatches(summary)\n",
    "\n",
    "# ---- Outputs ----\n",
    "# summary: one row per date with counts and lists of mismatches\n",
    "# mismatches_long: long table with one row per missing/excess matchup\n",
    "\n",
    "# Quick prints\n",
    "#print(\"Per-date summary (first 10 rows):\")\n",
    "#print(summary.head(10).to_string(index=False))\n",
    "\n",
    "#print(\"\\nLong-form mismatches (first 20 rows):\")\n",
    "#print(mismatches_long.head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e456608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More than 2 thirds of users place 3 or more bets in a day in regular season (caveat - no data on avg games per day being played)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5d3edbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1) Shared cleaning for BOTH training and unseen data ----------\n",
    "def clean_shared(df):\n",
    "    \"\"\"Return a cleaned copy with consistent text and a normalized bet_date.\"\"\"\n",
    "    out = df.copy()  # don't mutate caller\n",
    "\n",
    "    # Parse original timestamp column into pandas datetime (coerce bad values to NaT)\n",
    "    out[\"betdate\"] = pd.to_datetime(out[\"betdate\"], errors=\"coerce\", utc=False)\n",
    "\n",
    "    # Make event_description consistent: ensure string type, collapse spaces, trim ends\n",
    "    out[\"event_description\"] = (\n",
    "        out[\"event_description\"]\n",
    "          .astype(\"string\")\n",
    "          .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "          .str.strip()\n",
    "    )\n",
    "\n",
    "    # Split \"TeamA @ TeamB\" into AwayTeam/HomeTeam (safe even if some rows don't match)\n",
    "    if \"AwayTeam\" not in out.columns or \"HomeTeam\" not in out.columns:\n",
    "        tmp = out[\"event_description\"].str.split(\" @ \", n=1, expand=True)\n",
    "        out[\"AwayTeam\"] = tmp[0]\n",
    "        out[\"HomeTeam\"] = tmp[1]\n",
    "\n",
    "    # Create a normalized date column (midnight, tz-naive) for per-day evaluation\n",
    "    out[\"bet_date\"] = out[\"betdate\"].dt.normalize()\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------- 2) Train-only wrapper ----------\n",
    "def preprocess_train(df_train):\n",
    "    \"\"\"Apply shared cleaning + train-only hygiene/EDA, and return cleaned train frame.\"\"\"\n",
    "    t = clean_shared(df_train)\n",
    "\n",
    "    # (Optional but common) remove exact duplicate rows in TRAIN to avoid leakage/noise\n",
    "    # If duplicates truly represent identical records, keep='first' is reasonable.\n",
    "    before = len(t)\n",
    "    t = t.drop_duplicates().reset_index(drop=True)\n",
    "    after = len(t)\n",
    "    print(f\"[train] dropped exact duplicates: {before - after}\")\n",
    "\n",
    "    # Quick QA: how many rows match \"TeamA @ TeamB\" pattern (for data sanity)\n",
    "    pattern = re.compile(r\"^[A-Za-z0-9\\s.\\-']+ @ [A-Za-z0-9\\s.\\-']+$\")\n",
    "    match_rate = t[\"event_description\"].astype(str).str.match(pattern).mean()\n",
    "    print(f\"[train] 'A @ B' format match rate: {match_rate:.1%}\")\n",
    "\n",
    "    # Simple EDA: average daily bets per user and a tiny histogram\n",
    "    daily_counts = t.groupby([\"mask_id\", \"bet_date\"]).size()\n",
    "    avg_bets_per_user = daily_counts.groupby(\"mask_id\").mean()\n",
    "    bins = [0, 1, 2, 3, 4, float(\"inf\")]\n",
    "    labels = [\"1\", \"2\", \"3\", \"4\", \"4+\"]\n",
    "    dist = pd.cut(avg_bets_per_user, bins=bins, labels=labels).value_counts().sort_index()\n",
    "    dist_df = dist.to_frame(name=\"unique_users\")\n",
    "    dist_df[\"percent\"] = 100 * dist_df[\"unique_users\"] / dist_df[\"unique_users\"].sum()\n",
    "    print(\"[train] avg daily bets per user (buckets):\")\n",
    "    print(dist_df)\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "# ---------- 3) Validation/unseen wrapper ----------\n",
    "def preprocess_validation(df_validation):\n",
    "    \"\"\"Apply only the shared cleaning steps needed for inference/evaluation.\"\"\"\n",
    "    v = clean_shared(df_validation)\n",
    "\n",
    "    # NOTE: we typically do NOT drop duplicates in validation, because repeated bets\n",
    "    # can be real user behavior; your evaluation later collapses to unique items per day anyway.\n",
    "    # If your business rule says duplicates are true duplicates, you *may* drop them:\n",
    "    v = v.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Optional QA (same check as train, purely informative)\n",
    "    pattern = re.compile(r\"^[A-Za-z0-9\\s.\\-']+ @ [A-Za-z0-9\\s.\\-']+$\")\n",
    "    match_rate = v[\"event_description\"].astype(str).str.match(pattern).mean()\n",
    "    #print(f\"[validation] 'A @ B' format match rate: {match_rate:.1%}\")\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "073caa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] dropped exact duplicates: 12459\n",
      "[train] 'A @ B' format match rate: 100.0%\n",
      "[train] avg daily bets per user (buckets):\n",
      "    unique_users    percent\n",
      "1             63   9.077810\n",
      "2            161  23.198847\n",
      "3            125  18.011527\n",
      "4             83  11.959654\n",
      "4+           262  37.752161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(204392, 7)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean training data (shared steps + train-only hygiene/EDA)\n",
    "df_train = preprocess_train(df_train)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "93605b04-4db2-4f46-bb1e-dd8254e09ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _metrics_from_lists(true_set, recs, k=3):\n",
    "    \"\"\"Compute top-k ranking metrics (Hit@k, Precision@k, F1@k, NDCG@k) given a ground-truth set and a recommendation list.\"\"\"\n",
    "    recs_k = recs[:k]\n",
    "    hits = sum(int(i in true_set) for i in recs_k)\n",
    "    hitk = 1.0 if hits > 0 else 0.0\n",
    "    preck = hits / k\n",
    "    reck  = hits / max(1, len(true_set))\n",
    "    f1k   = 0.0 if (preck + reck) == 0 else 2*preck*reck/(preck+reck)\n",
    "    dcg = 0.0\n",
    "    for rank, iid in enumerate(recs_k, start=1):\n",
    "        if iid in true_set:\n",
    "            dcg += 1.0 / np.log2(rank + 1)\n",
    "    ideal = min(len(true_set), k)\n",
    "    idcg = sum(1.0/np.log2(r+1) for r in range(1, ideal+1)) if ideal>0 else 0.0\n",
    "    ndcgk = 0.0 if idcg == 0 else dcg / idcg\n",
    "    return hitk, preck, f1k, ndcgk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f6f0c4fa-54f2-4c90-a23e-1e8afb88bc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_away_home_series(s):\n",
    "    \"\"\"Convert matchup strings (e.g., 'A @ B', 'B vs A', 'X ## Y') into standardized 'AWAY ## HOME' format, else return None.\"\"\"\n",
    "    s = s.astype(str).str.strip()\n",
    "    out = pd.Series([None] * len(s), index=s.index, dtype=object)\n",
    "\n",
    "    # Case 1: AWAY @ HOME  (or 'at')\n",
    "    mask_at = s.str.contains(_RE_AT)\n",
    "    if mask_at.any():\n",
    "        parts = s.loc[mask_at].str.split(_RE_AT, n=1, expand=True)\n",
    "        away = parts[0].str.strip()\n",
    "        home = parts[1].str.strip()\n",
    "        out.loc[mask_at] = away + \" ## \" + home\n",
    "\n",
    "    # Case 2: HOME vs AWAY (or '&') -> flip to AWAY ## HOME\n",
    "    mask_vs = (~mask_at) & s.str.contains(_RE_VS)\n",
    "    if mask_vs.any():\n",
    "        parts = s.loc[mask_vs].str.split(_RE_VS, n=1, expand=True)\n",
    "        home = parts[0].str.strip()\n",
    "        away = parts[1].str.strip()\n",
    "        out.loc[mask_vs] = away + \" ## \" + home\n",
    "\n",
    "    # Case 3: already looks like 'X ## Y' -> keep (assume directional already)\n",
    "    mask_hash = out.isna() & s.str.contains(r\"##\")\n",
    "    if mask_hash.any():\n",
    "        cleaned = s.loc[mask_hash].str.replace(r\"\\s*##\\s*\", \" ## \", regex=True).str.strip()\n",
    "        out.loc[mask_hash] = cleaned\n",
    "\n",
    "    # Unparseable rows remain None\n",
    "    return out\n",
    "\n",
    "def build_dir_item(df):\n",
    "    \"\"\"\n",
    "    Return a Series of directional game ids in the form 'AWAY ## HOME'.\n",
    "\n",
    "    Preference:\n",
    "      1) vendor 'event_description' (e.g., 'LAL @ BOS' or 'BOS vs LAL')\n",
    "      2) away/home columns (common aliases)\n",
    "      3) 'item' column (attempt to parse; if already 'X ## Y', keep as-is)\n",
    "    \"\"\"\n",
    "    cmap = {c.lower(): c for c in df.columns}  # case-insensitive column lookup\n",
    "\n",
    "    # 1) event_description present: parse to AWAY ## HOME\n",
    "    if \"event_description\" in cmap:\n",
    "        return _to_away_home_series(df[cmap[\"event_description\"]])\n",
    "\n",
    "    # 2) try common away/home aliases\n",
    "    away_aliases = [\"awayteam\", \"away_team\", \"away\", \"visitor\"]\n",
    "    home_aliases = [\"hometeam\", \"home_team\", \"home\", \"homeclub\"]\n",
    "\n",
    "    away_col = next((cmap[a] for a in away_aliases if a in cmap), None)\n",
    "    home_col = next((cmap[h] for h in home_aliases if h in cmap), None)\n",
    "\n",
    "    if away_col is not None and home_col is not None:\n",
    "        away = df[away_col].astype(str).str.strip()\n",
    "        home = df[home_col].astype(str).str.strip()\n",
    "        return away + \" ## \" + home\n",
    "\n",
    "    # 3) last resort: try to parse 'item'; if it's already 'X ## Y', keep cleaned\n",
    "    if \"item\" in df.columns:\n",
    "        return _to_away_home_series(df[\"item\"])\n",
    "\n",
    "    raise KeyError(\"build_dir_item: need 'event_description' or Away/Home columns (or parsable 'item').\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "18b6de9c-cdc1-4b06-8770-f6bdccad80a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_k_for_Hyper_Optimization(model,\n",
    "               user_items,                   # CSR user×item matrix to pass into model.recommend\n",
    "               val_df, \n",
    "               pop_prior, \n",
    "               k=3,\n",
    "                            \n",
    "               low_decile_uids=None,         # set of uids to force popularity for (optional)\n",
    "               user_col=\"uid\",               # column name for user ids in val_df\n",
    "               item_col=\"iid\",               # column name for item ids in val_df\n",
    "               date_col=\"bet_date\",          # column name for date in val_df (normalized to days)\n",
    "               weights=(0.60, 0.30, 0.10),   # (w_ndcg, w_prec, w_cov)\n",
    "               return_preds=True,           # if True, also return a predictions table\n",
    "               item_names=None               # optional array mapping iid -> item string\n",
    "              ):\n",
    "    \"\"\"\n",
    "    Evaluate a recommender model at top-k using validation data, returning Hit@k,\n",
    "    Precision@k, NDCG@k, Coverage@k, and a weighted objective score (optionally\n",
    "    with per-user prediction tables).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        Trained recommender with a `.recommend(user, user_items, N)` method and\n",
    "        optional `user_factors`/`item_factors` attributes for shape checks.\n",
    "    user_items : scipy.sparse.csr_matrix\n",
    "        User–item interaction matrix used to generate recommendations.\n",
    "    val_df : pandas.DataFrame\n",
    "        Validation set containing user–item interactions to evaluate against.\n",
    "    pop_prior : array-like or None\n",
    "        Global item popularity prior (used as fallback for cold-start users).\n",
    "    k : int, default=3\n",
    "        Top-k cutoff for evaluation.\n",
    "    low_decile_uids : set of int, optional\n",
    "        User IDs to force onto popularity-based recommendations (simulating weak users).\n",
    "    user_col : str, default=\"uid\"\n",
    "        Column in `val_df` containing user IDs.\n",
    "    item_col : str, default=\"iid\"\n",
    "        Column in `val_df` containing item IDs.\n",
    "    date_col : str, default=\"bet_date\"\n",
    "        Column in `val_df` containing interaction dates (grouped by day).\n",
    "    weights : tuple of float, default=(0.60, 0.30, 0.10)\n",
    "        Weights applied to (NDCG, Precision, Coverage) when computing the objective.\n",
    "    return_preds : bool, default=True\n",
    "        If True, also return a DataFrame of top-k predictions per user/date.\n",
    "    item_names : array-like, optional\n",
    "        Optional mapping of item IDs to human-readable names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics : dict\n",
    "        Dictionary with average Hit@k, Precision@k, NDCG@k, Coverage@k,\n",
    "        weighted \"Objective\", and number of evaluated user-days.\n",
    "    preds : pandas.DataFrame, optional\n",
    "        If `return_preds=True`, a tidy table of predictions with columns:\n",
    "        [user_col, date_col, \"rank\", item_col, \"item\"].\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Cold-start or low-decile users fall back to popularity ordering if `pop_prior` is provided.\n",
    "    - Coverage measures how diverse the daily top-k recommendations are across users.\n",
    "    - The objective combines relevance and coverage into a single scalar for hyperparameter search.\n",
    "    \"\"\"\n",
    " \n",
    "    # --- Shapes & safety ---\n",
    "    mf_users = getattr(model, \"user_factors\", np.empty((0, 0))).shape[0]\n",
    "    mf_items = getattr(model, \"item_factors\", np.empty((0, 0))).shape[0]\n",
    "    UI_safe  = user_items[:mf_users, :]  # ensure #rows == mf_users\n",
    "\n",
    "    use_pop = (pop_prior is not None) and (len(pop_prior) == mf_items) and (float(np.max(pop_prior)) > 0)\n",
    "    pop_order = np.argsort(-pop_prior).tolist() if use_pop else []\n",
    "    low_decile_uids = set(low_decile_uids or [])\n",
    "\n",
    "    # --- Ground truth & slates ---\n",
    "    true_by_ud = (val_df.groupby([user_col, date_col])[item_col]\n",
    "                  .apply(lambda s: set(s.unique())).to_dict())\n",
    "    slate_by_day = (val_df.groupby(date_col)[item_col]\n",
    "                    .apply(lambda s: set(s.unique())).to_dict())\n",
    "\n",
    "    # --- Per-user recommendation lists (keys must match user_col values) ---\n",
    "    user_recs = {}\n",
    "    for u in pd.unique(val_df[user_col]):\n",
    "        u = int(u)\n",
    "        cold_or_low = (u < 0) or (u >= mf_users) or (u in low_decile_uids)\n",
    "        if cold_or_low:\n",
    "            user_recs[u] = pop_order[:] if use_pop else []\n",
    "            continue\n",
    "        try:\n",
    "            rec_i, _ = model.recommend(u, UI_safe, N=200,\n",
    "                                       filter_items=None,\n",
    "                                       filter_already_liked_items=False)\n",
    "            user_recs[u] = rec_i.tolist()\n",
    "        except IndexError:\n",
    "            user_recs[u] = pop_order[:] if use_pop else []\n",
    "\n",
    "    # --- Evaluate @k ---\n",
    "    hit_sum = prec_sum = ndcg_sum = 0.0\n",
    "    n = 0\n",
    "    unique_recs_by_day = {}  # day -> set(iids)\n",
    "    pred_rows = [] if return_preds else None\n",
    "\n",
    "    for (u, day), true_set in true_by_ud.items():\n",
    "        slate = slate_by_day.get(day, set())\n",
    "        if not slate:\n",
    "            continue\n",
    "        recs = [i for i in user_recs.get(int(u), []) if i in slate][:k]\n",
    "        if day not in unique_recs_by_day:\n",
    "            unique_recs_by_day[day] = set()\n",
    "        unique_recs_by_day[day].update(recs)\n",
    "\n",
    "        # predictions table (optional)\n",
    "        if return_preds and recs:\n",
    "            for r, iid in enumerate(recs, 1):\n",
    "                pred_rows.append({\n",
    "                    user_col: int(u),\n",
    "                    date_col: day,\n",
    "                    \"rank\": r,\n",
    "                    item_col: int(iid),\n",
    "                    \"item\": (None if item_names is None else item_names[int(iid)])\n",
    "                })\n",
    "\n",
    "        # metrics\n",
    "        hits = sum(1 for i in recs if i in true_set)\n",
    "        preck = hits / k\n",
    "\n",
    "        ideal = min(len(true_set), k)\n",
    "        if ideal > 0:\n",
    "            idcg = sum(1.0/np.log2(r+1) for r in range(1, ideal+1))\n",
    "            dcg  = sum(1.0/np.log2(r+1) for r, i in enumerate(recs, 1) if i in true_set)\n",
    "            ndcg = dcg / idcg\n",
    "        else:\n",
    "            ndcg = 0.0\n",
    "\n",
    "        hit_sum  += 1.0 if hits > 0 else 0.0\n",
    "        prec_sum += preck\n",
    "        ndcg_sum += ndcg\n",
    "        n += 1\n",
    "\n",
    "    # --- Aggregate + coverage + objective ---\n",
    "    if n == 0:\n",
    "        metrics = {f\"Hit@{k}\": 0.0, f\"Precision@{k}\": 0.0, f\"NDCG@{k}\": 0.0,\n",
    "                   f\"Coverage@{k}\": 0.0, \"Objective\": 0.0, \"n_eval\": 0}\n",
    "        preds = (pd.DataFrame(pred_rows) if return_preds else None)\n",
    "        return (metrics, preds) if return_preds else metrics\n",
    "\n",
    "    ndcg = ndcg_sum / n\n",
    "    prec = prec_sum / n\n",
    "    hit  = hit_sum  / n\n",
    "    total_recs = n * k\n",
    "    unique_day_item = sum(len(s) for s in unique_recs_by_day.values())\n",
    "    coverage = unique_day_item / max(1, total_recs)\n",
    "\n",
    "    w_ndcg, w_prec, w_cov = weights\n",
    "    objective = w_ndcg * ndcg + w_prec * prec + w_cov * coverage\n",
    "\n",
    "    metrics = {f\"Hit@{k}\": hit,\n",
    "               f\"Precision@{k}\": prec,\n",
    "               f\"NDCG@{k}\": ndcg,\n",
    "               f\"Coverage@{k}\": coverage,\n",
    "               \"Objective\": objective,\n",
    "               \"n_eval\": n}\n",
    "\n",
    "    preds = (pd.DataFrame(pred_rows).sort_values([user_col, date_col, \"rank\"])\n",
    "             if return_preds else None)\n",
    "    return (metrics, preds) if return_preds else metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7442f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mask_id</th>\n",
       "      <th>betdate</th>\n",
       "      <th>event_description</th>\n",
       "      <th>wager_amount</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>bet_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>751771</td>\n",
       "      <td>2024-10-22 00:11:01</td>\n",
       "      <td>Minnesota Timberwolves @ Los Angeles Lakers</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>Minnesota Timberwolves</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>2024-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>751771</td>\n",
       "      <td>2024-10-22 00:11:20</td>\n",
       "      <td>Minnesota Timberwolves @ Los Angeles Lakers</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>Minnesota Timberwolves</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>2024-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>751771</td>\n",
       "      <td>2024-10-22 00:23:25</td>\n",
       "      <td>Minnesota Timberwolves @ Los Angeles Lakers</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>Minnesota Timberwolves</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>2024-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>751771</td>\n",
       "      <td>2024-10-22 00:24:13</td>\n",
       "      <td>Minnesota Timberwolves @ Los Angeles Lakers</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>Minnesota Timberwolves</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>2024-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>751771</td>\n",
       "      <td>2024-10-22 00:25:32</td>\n",
       "      <td>Minnesota Timberwolves @ Los Angeles Lakers</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>Minnesota Timberwolves</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>2024-10-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mask_id             betdate                            event_description  \\\n",
       "0   751771 2024-10-22 00:11:01  Minnesota Timberwolves @ Los Angeles Lakers   \n",
       "1   751771 2024-10-22 00:11:20  Minnesota Timberwolves @ Los Angeles Lakers   \n",
       "2   751771 2024-10-22 00:23:25  Minnesota Timberwolves @ Los Angeles Lakers   \n",
       "3   751771 2024-10-22 00:24:13  Minnesota Timberwolves @ Los Angeles Lakers   \n",
       "4   751771 2024-10-22 00:25:32  Minnesota Timberwolves @ Los Angeles Lakers   \n",
       "\n",
       "   wager_amount                AwayTeam            HomeTeam   bet_date  \n",
       "0      1.666667  Minnesota Timberwolves  Los Angeles Lakers 2024-10-22  \n",
       "1      5.000000  Minnesota Timberwolves  Los Angeles Lakers 2024-10-22  \n",
       "2      0.833333  Minnesota Timberwolves  Los Angeles Lakers 2024-10-22  \n",
       "3      0.909091  Minnesota Timberwolves  Los Angeles Lakers 2024-10-22  \n",
       "4      0.833333  Minnesota Timberwolves  Los Angeles Lakers 2024-10-22  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b8af0713-09a6-4d12-85f4-5acc246020fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COVERAGE] VAL users unseen in TRAIN: 43 / 496 (8.7%)\n",
      "[COVERAGE] VAL items unseen in TRAIN: 108 / 207 (52.2%)\n",
      "[COVERAGE] sample unseen user ids: [134111, 139018, 155268, 155662, 226897, 232637, 236153, 265134, 277155, 298583]\n",
      "[COVERAGE] sample unseen item ids: ['Atlanta Hawks ## Dallas Mavericks', 'Atlanta Hawks ## Houston Rockets', 'Atlanta Hawks ## Philadelphia 76ers', 'Boston Celtics ## Memphis Grizzlies', 'Boston Celtics ## Phoenix Suns', 'Boston Celtics ## Portland Trail Blazers', 'Boston Celtics ## Sacramento Kings', 'Boston Celtics ## San Antonio Spurs', 'Boston Celtics ## Utah Jazz', 'Brooklyn Nets ## Dallas Mavericks']\n",
      "Total configs: 48\n",
      "TUNING (implicit BPR; non-directional items; UI CSR for recommend):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084d0a871b634073a769839a3308498c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.001, 'regularization': 1e-05, 'iterations': 40} -> Hit@3=0.4654  Prec@3=0.1905  NDCG@3=0.2855  Coverage@3=0.0054  Objective=0.2290\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5b0c11eb274a5193fe2633043bfa5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.001, 'regularization': 0.0001, 'iterations': 40} -> Hit@3=0.4440  Prec@3=0.1766  NDCG@3=0.2871  Coverage@3=0.0052  Objective=0.2258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43586068109948a48d1f3da2e2959700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.001, 'regularization': 0.001, 'iterations': 40} -> Hit@3=0.4447  Prec@3=0.1844  NDCG@3=0.3010  Coverage@3=0.0053  Objective=0.2365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846a1dc1c86144af89eef3469d8fae5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.001, 'regularization': 0.01, 'iterations': 40} -> Hit@3=0.4457  Prec@3=0.1937  NDCG@3=0.2870  Coverage@3=0.0054  Objective=0.2308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6572eb6d1eb4afd984b1904d1ecee01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.01, 'regularization': 1e-05, 'iterations': 40} -> Hit@3=0.4661  Prec@3=0.1909  NDCG@3=0.2848  Coverage@3=0.0113  Objective=0.2293\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964830acdc6e403e9306d38ee5f49fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.01, 'regularization': 0.0001, 'iterations': 40} -> Hit@3=0.4861  Prec@3=0.1950  NDCG@3=0.2975  Coverage@3=0.0118  Objective=0.2382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db282c24756a4b278750c65b6af750ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.01, 'regularization': 0.001, 'iterations': 40} -> Hit@3=0.4657  Prec@3=0.1868  NDCG@3=0.2834  Coverage@3=0.0118  Objective=0.2273\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3dff13f9b04662bc9fd046b08e2a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.01, 'regularization': 0.01, 'iterations': 40} -> Hit@3=0.4895  Prec@3=0.1987  NDCG@3=0.3022  Coverage@3=0.0111  Objective=0.2420\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af86e39af004027a4957c16112e787f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.001, 'regularization': 1e-05, 'iterations': 60} -> Hit@3=0.4532  Prec@3=0.1808  NDCG@3=0.2840  Coverage@3=0.0053  Objective=0.2252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5cf807cc0c46cca131a531ec885862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.001, 'regularization': 0.0001, 'iterations': 60} -> Hit@3=0.5431  Prec@3=0.2255  NDCG@3=0.3420  Coverage@3=0.0059  Objective=0.2734\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed113f2807f4e898fc1d15ee7498ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.001, 'regularization': 0.001, 'iterations': 60} -> Hit@3=0.5173  Prec@3=0.2037  NDCG@3=0.3188  Coverage@3=0.0057  Objective=0.2530\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e051ff3e1ec7410991ea6c5d1c06c251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.001, 'regularization': 0.01, 'iterations': 60} -> Hit@3=0.4756  Prec@3=0.1978  NDCG@3=0.3016  Coverage@3=0.0052  Objective=0.2408\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4f150b62d5434ba24b05574db865b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.01, 'regularization': 1e-05, 'iterations': 60} -> Hit@3=0.4745  Prec@3=0.1946  NDCG@3=0.2956  Coverage@3=0.0166  Objective=0.2374\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08f153150ed45b69257c35d35ed5e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.01, 'regularization': 0.0001, 'iterations': 60} -> Hit@3=0.4532  Prec@3=0.1841  NDCG@3=0.2823  Coverage@3=0.0153  Objective=0.2261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb8e1f2479c487a910f2e892e34cfbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.01, 'regularization': 0.001, 'iterations': 60} -> Hit@3=0.4559  Prec@3=0.1862  NDCG@3=0.2834  Coverage@3=0.0156  Objective=0.2275\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ac077821a44cdcb4b1e90ddb9b4663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.01, 'regularization': 0.01, 'iterations': 60} -> Hit@3=0.4661  Prec@3=0.1924  NDCG@3=0.2921  Coverage@3=0.0153  Objective=0.2345\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9deffda8f84a0ab9d01879287adaf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.001, 'regularization': 1e-05, 'iterations': 80} -> Hit@3=0.4986  Prec@3=0.2068  NDCG@3=0.3074  Coverage@3=0.0058  Objective=0.2471\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe08ea1bd09419db17e2e591e348cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.001, 'regularization': 1e-05, 'iterations': 40} -> Hit@3=0.4722  Prec@3=0.1981  NDCG@3=0.3022  Coverage@3=0.0054  Objective=0.2413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa192fee836947ee972536cda392b023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.001, 'regularization': 0.0001, 'iterations': 80} -> Hit@3=0.5166  Prec@3=0.1974  NDCG@3=0.3170  Coverage@3=0.0057  Objective=0.2500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bc9c26041f4b7ca5ddfb18d73a43d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.001, 'regularization': 0.0001, 'iterations': 40} -> Hit@3=0.4396  Prec@3=0.1718  NDCG@3=0.2810  Coverage@3=0.0049  Objective=0.2206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea478fcaec8b4ca5a2518cfd6b51c69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.001, 'regularization': 0.001, 'iterations': 80} -> Hit@3=0.4749  Prec@3=0.1964  NDCG@3=0.2973  Coverage@3=0.0055  Objective=0.2379\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4684e5c22ee4cd2942228ce40c6481d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.001, 'regularization': 0.001, 'iterations': 40} -> Hit@3=0.4647  Prec@3=0.1808  NDCG@3=0.2989  Coverage@3=0.0051  Objective=0.2341\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221a88f5dd7f4c3c847833a59c06d5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.001, 'regularization': 0.01, 'iterations': 80} -> Hit@3=0.4844  Prec@3=0.2007  NDCG@3=0.3139  Coverage@3=0.0054  Objective=0.2491\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c248a5c82a9446af917ad8ca78665122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.001, 'regularization': 0.01, 'iterations': 40} -> Hit@3=0.4379  Prec@3=0.1754  NDCG@3=0.2865  Coverage@3=0.0054  Objective=0.2250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583943f5ed8f48f98d5ceb44189ab3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.01, 'regularization': 1e-05, 'iterations': 80} -> Hit@3=0.4654  Prec@3=0.1888  NDCG@3=0.2875  Coverage@3=0.0179  Objective=0.2309\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137abfe456cd49d6bf4074f03cfd168b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.01, 'regularization': 1e-05, 'iterations': 40} -> Hit@3=0.4932  Prec@3=0.2026  NDCG@3=0.3077  Coverage@3=0.0122  Objective=0.2466\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a86204c1254d88b2b1462f88deae18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.01, 'regularization': 0.0001, 'iterations': 80} -> Hit@3=0.4532  Prec@3=0.1851  NDCG@3=0.2809  Coverage@3=0.0173  Objective=0.2258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758a80d8bb844bcf8cf051dbb187fe30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.01, 'regularization': 0.0001, 'iterations': 40} -> Hit@3=0.4830  Prec@3=0.1960  NDCG@3=0.2973  Coverage@3=0.0117  Objective=0.2383\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df7bba6adb2432f905379b0712b2986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.01, 'regularization': 0.001, 'iterations': 80} -> Hit@3=0.4613  Prec@3=0.1904  NDCG@3=0.2864  Coverage@3=0.0173  Objective=0.2307\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336a01576d1f44738d98d065283919b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.01, 'regularization': 0.001, 'iterations': 40} -> Hit@3=0.4681  Prec@3=0.1911  NDCG@3=0.2871  Coverage@3=0.0109  Objective=0.2307\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9148a7efc66e47069a8bf56bf2c703f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 32, 'learning_rate': 0.01, 'regularization': 0.01, 'iterations': 80} -> Hit@3=0.4508  Prec@3=0.1826  NDCG@3=0.2800  Coverage@3=0.0172  Objective=0.2245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26ee8a634fa403b969c4336c5057f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.01, 'regularization': 0.01, 'iterations': 40} -> Hit@3=0.4684  Prec@3=0.1916  NDCG@3=0.2854  Coverage@3=0.0109  Objective=0.2298\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4214c0f11f4159b66d552e92208dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.001, 'regularization': 1e-05, 'iterations': 60} -> Hit@3=0.4749  Prec@3=0.1903  NDCG@3=0.2981  Coverage@3=0.0053  Objective=0.2365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e524d4d51949a494e5dd04740f8072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.001, 'regularization': 0.0001, 'iterations': 60} -> Hit@3=0.4657  Prec@3=0.1852  NDCG@3=0.3002  Coverage@3=0.0055  Objective=0.2362\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410f9b65e1f440baabfc618a49d6e3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.001, 'regularization': 0.001, 'iterations': 60} -> Hit@3=0.4742  Prec@3=0.1835  NDCG@3=0.3012  Coverage@3=0.0051  Objective=0.2363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c13ebcf66064b59bb9cd900b4e08681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.001, 'regularization': 0.01, 'iterations': 60} -> Hit@3=0.4053  Prec@3=0.1712  NDCG@3=0.2675  Coverage@3=0.0051  Objective=0.2123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c62bed221b6453aabd540015d737d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.01, 'regularization': 1e-05, 'iterations': 60} -> Hit@3=0.4718  Prec@3=0.1924  NDCG@3=0.2910  Coverage@3=0.0161  Objective=0.2339\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddafac0979344884913f4d9d44655e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.01, 'regularization': 0.0001, 'iterations': 60} -> Hit@3=0.4654  Prec@3=0.1912  NDCG@3=0.2891  Coverage@3=0.0153  Objective=0.2324\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de78698d1a84d6097406bc1d8499e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.01, 'regularization': 0.001, 'iterations': 60} -> Hit@3=0.4644  Prec@3=0.1916  NDCG@3=0.2899  Coverage@3=0.0154  Objective=0.2330\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8526151f989a4ff9b13ad3bd86cfb92c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.01, 'regularization': 0.01, 'iterations': 60} -> Hit@3=0.4701  Prec@3=0.1926  NDCG@3=0.2950  Coverage@3=0.0147  Objective=0.2363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e7e21ed9f7461898fd40343a3fdaf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.001, 'regularization': 1e-05, 'iterations': 80} -> Hit@3=0.4837  Prec@3=0.1961  NDCG@3=0.2854  Coverage@3=0.0055  Objective=0.2306\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892fba56045f44e2b1c67cde42fcd10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.001, 'regularization': 0.0001, 'iterations': 80} -> Hit@3=0.4742  Prec@3=0.1973  NDCG@3=0.3048  Coverage@3=0.0052  Objective=0.2426\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e285b7f4047043d4b99a48bf012b9482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.001, 'regularization': 0.001, 'iterations': 80} -> Hit@3=0.4786  Prec@3=0.1991  NDCG@3=0.3023  Coverage@3=0.0053  Objective=0.2416\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd0817d1dcf4aceb28fa3e2475dfaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.001, 'regularization': 0.01, 'iterations': 80} -> Hit@3=0.4956  Prec@3=0.1914  NDCG@3=0.3163  Coverage@3=0.0053  Objective=0.2478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da11fe4288c48d185b2fd2597b30cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.01, 'regularization': 1e-05, 'iterations': 80} -> Hit@3=0.4616  Prec@3=0.1900  NDCG@3=0.2880  Coverage@3=0.0171  Objective=0.2315\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb852e9010d477c9e595166719a0f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.01, 'regularization': 0.0001, 'iterations': 80} -> Hit@3=0.4576  Prec@3=0.1896  NDCG@3=0.2891  Coverage@3=0.0171  Objective=0.2320\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40b381321ab44e2b0dda0bf06d0e4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.01, 'regularization': 0.001, 'iterations': 80} -> Hit@3=0.4650  Prec@3=0.1912  NDCG@3=0.2922  Coverage@3=0.0171  Objective=0.2344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb4c0fc3286446c84d4c631ecf982d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] model.user_factors=762 but IU_train.shape[1]=651\n",
      "model users: 762  | UI rows: 651  | max val uid: 649  | items: 651  | IU items: 762\n",
      "[INFO] Dropped users: 10 / 413 (2.4%), items: 17 / 99 (17.2%)\n",
      "  cfg={'factors': 64, 'learning_rate': 0.01, 'regularization': 0.01, 'iterations': 80} -> Hit@3=0.4640  Prec@3=0.1909  NDCG@3=0.2890  Coverage@3=0.0172  Objective=0.2324\n",
      "\n",
      "BEST ON VALIDATION:\n",
      "  cfg={'factors': 32, 'learning_rate': 0.001, 'regularization': 0.0001, 'iterations': 60}\n",
      "  Hit@3=0.5431 | Precision@3=0.2255 | NDCG@3=0.3420 | Coverage@3=0.0059 | Objective=0.2734\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0976e4b3d07e48c78b3c9fd2c37cd168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[validation] 'A @ B' format match rate: 100.0%\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30243 entries, 0 to 30242\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   mask_id              30243 non-null  int64         \n",
      " 1   betdate              30243 non-null  datetime64[ns]\n",
      " 2   event_description    30243 non-null  string        \n",
      " 3   wager_amount         30243 non-null  float64       \n",
      " 4   bet_date             30243 non-null  object        \n",
      " 5   Scheduled Game Date  28517 non-null  datetime64[ns]\n",
      " 6   Offical Game Date    28468 non-null  datetime64[ns]\n",
      " 7   AwayTeam             30243 non-null  string        \n",
      " 8   HomeTeam             30243 non-null  string        \n",
      "dtypes: datetime64[ns](3), float64(1), int64(1), object(1), string(3)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "def run_bpr_training_and_evaluation(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_validation: pd.DataFrame,\n",
    "    preprocess_validation: Callable[[pd.DataFrame], pd.DataFrame],\n",
    "    evaluate_k_for_Hyper_Optimization: Callable[..., Dict[str, float]],\n",
    "    build_dir_item: Callable[[pd.DataFrame], pd.Series],\n",
    "    factors_grid=(32, 64),\n",
    "    learning_rates=(0.001, 0.01),\n",
    "    regularizations=(1e-5, 1e-4, 1e-3, 1e-2),\n",
    "    iterations_grid=(40, 60, 80),\n",
    "    k_eval: int = 3,\n",
    "    objective_weights: Tuple[float, float, float] = (0.60, 0.30, 0.10),\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    End-to-end pipeline that: prepares data, performs a temporal split, encodes users/items,\n",
    "    builds implicit matrices, runs grid-search for implicit BPR, reports the best config,\n",
    "    then refits a final BPR model on all positives and preprocesses validation for downstream use.\n",
    "\n",
    "    Steps & behavior (mirrors the provided script exactly):\n",
    "      1) Prepare: copy, parse bet_date, build canonical 'item' via build_dir_item, keep positives (wager_amount > 0).\n",
    "      2) Temporal split: last 20% by bet_date -> validation; first 80% -> train; print user/item coverage diagnostics.\n",
    "      3) Encode users/items from TRAIN ONLY; create mapping DataFrames and map train/val into train space.\n",
    "      4) Popularity prior from TRAIN (normalized 0..1) for evaluation fallback; pop_order computed (desc).\n",
    "      5) Build sparse matrices:\n",
    "         - TRAIN matrices: IU_train (items×users) and UI_train (users×items) with **binary** weights (standard BPR).\n",
    "         - Cast CSR internals to int32 and guard UI rows to match model.user_factors when recommending.\n",
    "      6) Hyperparameter tuning:\n",
    "         - Grid: Cartesian product of factors, learning_rates, regularizations, iterations.\n",
    "         - Fit each config on IU_train; align validation to model shapes; evaluate at k using provided evaluator.\n",
    "         - Track and print Hit@k, Precision@k, NDCG@k, Coverage@k, and weighted Objective; keep best config/model.\n",
    "      7) Report best config and metrics.\n",
    "      8) Final refit on **ALL** positives:\n",
    "         - Factorize across ALL positives; build IU_all/UI_all with **log1p(wager_amount)** weights (as in original).\n",
    "         - Train final BayesianPersonalizedRanking on IU_all.\n",
    "         - Build user/item index maps and iid_to_item reverse map.\n",
    "      9) Preprocess the provided df_validation (copy), ensure bet_date is datetime.date, and print .info().\n",
    "\n",
    "    Returns a dict with: best config/metrics/model, train/val artifacts, final model, matrices, mappings, and cleaned validation.\n",
    "    \"\"\"\n",
    "    # --- 1) Prepare data ---\n",
    "    df = df_train.copy()\n",
    "    df[\"bet_date\"] = pd.to_datetime(df[\"bet_date\"])\n",
    "    df[\"item\"] = build_dir_item(df)  # canonical item id \"AWAY ## HOME\"\n",
    "    pos = df.loc[df[\"wager_amount\"] > 0]\n",
    "\n",
    "    # --- Split: last 10% dates as validation ---\n",
    "    cut = pos[\"bet_date\"].quantile(0.80)\n",
    "    train_pos = pos.loc[pos[\"bet_date\"] < cut].copy()\n",
    "    val_pos = pos.loc[pos[\"bet_date\"] >= cut].copy()\n",
    "\n",
    "    # --- Coverage diagnostics ---\n",
    "    train_users_raw = set(train_pos[\"mask_id\"].unique())\n",
    "    val_users_raw = set(val_pos[\"mask_id\"].unique())\n",
    "    train_items_raw = set(train_pos[\"item\"].unique())\n",
    "    val_items_raw = set(val_pos[\"item\"].unique())\n",
    "\n",
    "    val_users_unseen = val_users_raw - train_users_raw\n",
    "    val_items_unseen = val_items_raw - train_items_raw\n",
    "\n",
    "    print(\n",
    "        f\"[COVERAGE] VAL users unseen in TRAIN: {len(val_users_unseen)} / {len(val_users_raw)} \"\n",
    "        f\"({(len(val_users_unseen)/max(1,len(val_users_raw))):.1%})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"[COVERAGE] VAL items unseen in TRAIN: {len(val_items_unseen)} / {len(val_items_raw)} \"\n",
    "        f\"({(len(val_items_unseen)/max(1,len(val_items_raw))):.1%})\"\n",
    "    )\n",
    "    _unseen_users_list = sorted(list(val_users_unseen))[:10]\n",
    "    _unseen_items_list = sorted(list(val_items_unseen))[:10]\n",
    "    print(f\"[COVERAGE] sample unseen user ids: {_unseen_users_list}\")\n",
    "    print(f\"[COVERAGE] sample unseen item ids: {_unseen_items_list}\")\n",
    "\n",
    "    # --- 2) Encode from TRAIN ONLY ---\n",
    "    u_codes, users = pd.factorize(train_pos[\"mask_id\"], sort=True)\n",
    "    i_codes, items = pd.factorize(train_pos[\"item\"], sort=True)\n",
    "    u_map = pd.DataFrame({\"mask_id\": users, \"uid\": np.arange(len(users), dtype=np.int32)})\n",
    "    i_map = pd.DataFrame({\"item\": items, \"iid\": np.arange(len(items), dtype=np.int32)})\n",
    "\n",
    "    def to_train_space(df_: pd.DataFrame) -> pd.DataFrame:\n",
    "        return (\n",
    "            df_.merge(u_map, on=\"mask_id\", how=\"inner\")\n",
    "               .merge(i_map, on=\"item\", how=\"inner\")\n",
    "        )\n",
    "\n",
    "    train_m = to_train_space(train_pos)\n",
    "    val_m = to_train_space(val_pos)\n",
    "\n",
    "    # --- Popularity prior from TRAIN (0..1) ---\n",
    "    _item_counts = train_m.groupby(\"iid\").size()\n",
    "    pop_prior = _item_counts.reindex(range(len(items)), fill_value=0).to_numpy(dtype=np.float32)\n",
    "    if pop_prior.max() > 0:\n",
    "        pop_prior = pop_prior / pop_prior.max()\n",
    "    else:\n",
    "        pop_prior = np.zeros_like(pop_prior, dtype=np.float32)\n",
    "\n",
    "    pop_order = np.argsort(-pop_prior)\n",
    "\n",
    "    n_users, n_items = len(users), len(items)\n",
    "    if (n_users == 0) or (n_items == 0) or train_m.empty or val_m.empty:\n",
    "        raise ValueError(\"Empty train/val after split+encoding. Adjust split or check coverage.\")\n",
    "\n",
    "    # --- 3) Sparse matrices for TRAIN (binary weights for standard BPR) ---\n",
    "    W = train_m[\"wager_amount\"].astype(bool).astype(np.float32).to_numpy()\n",
    "    IU_train = sp.coo_matrix(\n",
    "        (W, (train_m[\"iid\"].to_numpy(), train_m[\"uid\"].to_numpy())),\n",
    "        shape=(n_items, n_users),\n",
    "        dtype=np.float32,\n",
    "    ).tocsr()\n",
    "    UI_train = IU_train.T.tocsr()\n",
    "\n",
    "    for M in (IU_train, UI_train):\n",
    "        M.indptr = M.indptr.astype(np.int32, copy=False)\n",
    "        M.indices = M.indices.astype(np.int32, copy=False)\n",
    "\n",
    "    if UI_train.shape[0] != IU_train.shape[1]:\n",
    "        UI_train = UI_train[:IU_train.shape[1], :]\n",
    "\n",
    "    # --- 4) Hyperparameter tuning ---\n",
    "    FACTORS = list(factors_grid)\n",
    "    LEARNING_RATES = list(learning_rates)\n",
    "    REGULARIZATIONS = list(regularizations)\n",
    "    ITERATIONS = list(iterations_grid)\n",
    "\n",
    "    grid = [\n",
    "        dict(factors=f, learning_rate=lr, regularization=reg, iterations=it)\n",
    "        for f, lr, reg, it in product(FACTORS, LEARNING_RATES, REGULARIZATIONS, ITERATIONS)\n",
    "    ]\n",
    "    grid.sort(key=lambda p: (p[\"factors\"] * p[\"iterations\"], p[\"learning_rate\"], p[\"regularization\"]))\n",
    "\n",
    "    print(f\"Total configs: {len(grid)}\")  # 2*2*4*3 = 48 by default\n",
    "\n",
    "    best: Dict[str, Any] = {\"score\": -1.0, \"metrics\": None, \"cfg\": None, \"model\": None}\n",
    "\n",
    "    print(\"TUNING (implicit BPR; non-directional items; UI CSR for recommend):\")\n",
    "    for p in grid:\n",
    "        model = BayesianPersonalizedRanking(\n",
    "            factors=p[\"factors\"],\n",
    "            learning_rate=p[\"learning_rate\"],\n",
    "            regularization=p[\"regularization\"],\n",
    "            iterations=p[\"iterations\"],\n",
    "            verify_negative_samples=True,\n",
    "            num_threads=0,  # 0=all cores; set 1 for deterministic single-thread if desired\n",
    "        )\n",
    "        model.fit(IU_train)\n",
    "\n",
    "        # Sanity checks: model vs matrices\n",
    "        mf_users = getattr(model, \"user_factors\", np.empty((0, 0))).shape[0]\n",
    "        mf_items = getattr(model, \"item_factors\", np.empty((0, 0))).shape[0]\n",
    "\n",
    "        if mf_users != IU_train.shape[1]:\n",
    "            print(f\"[WARN] model.user_factors={mf_users} but IU_train.shape[1]={IU_train.shape[1]}\")\n",
    "\n",
    "        UI_train_safe = UI_train[:mf_users, :] if UI_train.shape[0] != mf_users else UI_train\n",
    "\n",
    "        print(\n",
    "            \"model users:\", mf_users, \" | UI rows:\", UI_train_safe.shape[0],\n",
    "            \" | max val uid:\", int(val_m[\"uid\"].max()),\n",
    "            \" | items:\", mf_items, \" | IU items:\", IU_train.shape[0]\n",
    "        )\n",
    "\n",
    "        # Align validation DF to the trained model's user/item ranges\n",
    "        val_eval = val_m[(val_m[\"uid\"] < mf_users) & (val_m[\"iid\"] < mf_items)].copy()\n",
    "        if val_eval.empty:\n",
    "            print(\"[WARN] validation set became empty after alignment; skipping config\")\n",
    "            continue\n",
    "\n",
    "        # Diagnostics: show how much we dropped after alignment\n",
    "        u0, u1 = val_m[\"uid\"].nunique(), val_eval[\"uid\"].nunique()\n",
    "        i0, i1 = val_m[\"iid\"].nunique(), val_eval[\"iid\"].nunique()\n",
    "        if (u0 != u1) or (i0 != i1):\n",
    "            print(\n",
    "                f\"[INFO] Dropped users: {u0 - u1} / {u0} ({(u0-u1)/max(1,u0):.1%}), \"\n",
    "                f\"items: {i0 - i1} / {i0} ({(i0-i1)/max(1,i0):.1%})\"\n",
    "            )\n",
    "\n",
    "        metrics = evaluate_k_for_Hyper_Optimization(\n",
    "            model=model,\n",
    "            user_items=UI_train_safe,\n",
    "            val_df=val_eval,               # must have columns uid, iid, bet_date\n",
    "            pop_prior=pop_prior,\n",
    "            k=k_eval,\n",
    "            low_decile_uids=None,          # or set()\n",
    "            user_col=\"uid\",\n",
    "            item_col=\"iid\",\n",
    "            date_col=\"bet_date\",\n",
    "            weights=objective_weights,\n",
    "            return_preds=False,            # tuning: metrics only\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"  cfg={p} -> Hit@{k_eval}={metrics[f'Hit@{k_eval}']:.4f}  \"\n",
    "            f\"Prec@{k_eval}={metrics[f'Precision@{k_eval}']:.4f}  \"\n",
    "            f\"NDCG@{k_eval}={metrics[f'NDCG@{k_eval}']:.4f}  \"\n",
    "            f\"Coverage@{k_eval}={metrics[f'Coverage@{k_eval}']:.4f}  \"\n",
    "            f\"Objective={metrics['Objective']:.4f}\"\n",
    "        )\n",
    "\n",
    "        if metrics[\"Objective\"] > best[\"score\"]:\n",
    "            best[\"score\"] = metrics[\"Objective\"]\n",
    "            best[\"metrics\"] = metrics\n",
    "            best[\"cfg\"] = p\n",
    "            best[\"model\"] = model\n",
    "\n",
    "    # --- Report best ---\n",
    "    m = best[\"metrics\"]\n",
    "    #print(\"\\nBEST ON VALIDATION:\")\n",
    "    #print(f\"  cfg={best['cfg']}\")\n",
    "#     print(\n",
    "#         f\"  Hit@{k_eval}={m[f'Hit@{k_eval}']:.4f} | \"\n",
    "#         f\"Precision@{k_eval}={m[f'Precision@{k_eval}']:.4f} | \"\n",
    "#         f\"NDCG@{k_eval}={m[f'NDCG@{k_eval}']:.4f} | \"\n",
    "#         f\"Coverage@{k_eval}={m[f'Coverage@{k_eval}']:.4f} | \"\n",
    "#         f\"Objective={m['Objective']:.4f}\")\n",
    "\n",
    "    # --- Final training on ALL positives  ---\n",
    "    u_codes_all, users_all = pd.factorize(pos[\"mask_id\"], sort=True)\n",
    "    i_codes_all, items_all = pd.factorize(pos[\"item\"], sort=True)\n",
    "    #W_all = np.log1p(pos[\"wager_amount\"].astype(np.float32).to_numpy())\n",
    "    W_all=pos[\"wager_amount\"].astype(bool).astype(np.float32).to_numpy()\n",
    "    \n",
    "\n",
    "    IU_all = sp.coo_matrix(\n",
    "        (W_all, (i_codes_all, u_codes_all)),\n",
    "        shape=(len(items_all), len(users_all)),\n",
    "        dtype=np.float32,\n",
    "    ).tocsr()\n",
    "    UI_all = IU_all.T.tocsr()\n",
    "\n",
    "    for M in (IU_all, UI_all):\n",
    "        M.indptr = M.indptr.astype(np.int32, copy=False)\n",
    "        M.indices = M.indices.astype(np.int32, copy=False)\n",
    "\n",
    "    final_model = BayesianPersonalizedRanking(\n",
    "        factors=best[\"cfg\"][\"factors\"],\n",
    "        learning_rate=best[\"cfg\"][\"learning_rate\"],\n",
    "        regularization=best[\"cfg\"][\"regularization\"],\n",
    "        iterations=best[\"cfg\"][\"iterations\"],\n",
    "        verify_negative_samples=True,\n",
    "        num_threads=0,\n",
    "    )\n",
    "    final_model.fit(IU_all)\n",
    "\n",
    "    user_index_all = {u: i for i, u in enumerate(users_all)}\n",
    "    item_index_all = {it: i for i, it in enumerate(items_all)}\n",
    "    iid_to_item = {i: it for it, i in item_index_all.items()}\n",
    "\n",
    "    # --- Clean validation/unseen data (shared steps) ---\n",
    "    df_val_clean = preprocess_validation(df_validation.copy())\n",
    "    df_val_clean[\"bet_date\"] = pd.to_datetime(df_val_clean[\"bet_date\"]).dt.date\n",
    "    df_val_clean.info()\n",
    "\n",
    "    return {\n",
    "        # tuning artifacts\n",
    "        \"best\": best,                          # {\"score\",\"metrics\",\"cfg\",\"model\"}\n",
    "        \"UI_train\": UI_train,\n",
    "        \"IU_train\": IU_train,\n",
    "        \"train_m\": train_m,\n",
    "        \"val_m\": val_m,\n",
    "        \"pop_prior\": pop_prior,\n",
    "        \"pop_order\": pop_order,\n",
    "        \"users\": users,\n",
    "        \"items\": items,\n",
    "        # final model artifacts\n",
    "        \"final_model\": final_model,\n",
    "        \"UI_all\": UI_all,\n",
    "        \"IU_all\": IU_all,\n",
    "        \"users_all\": users_all,\n",
    "        \"items_all\": items_all,\n",
    "        \"user_index_all\": user_index_all,\n",
    "        \"item_index_all\": item_index_all,\n",
    "        \"iid_to_item\": iid_to_item,\n",
    "        # cleaned validation\n",
    "        \"df_validation\": df_val_clean,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "##Call the function and unpack\n",
    "\n",
    "results = run_bpr_training_and_evaluation(\n",
    "    df_train=df_train,\n",
    "    df_validation=df_validation,\n",
    "    preprocess_validation=preprocess_validation,\n",
    "    evaluate_k_for_Hyper_Optimization=evaluate_k_for_Hyper_Optimization,\n",
    "    build_dir_item=build_dir_item,\n",
    "    factors_grid=(32, 64),\n",
    "    learning_rates=(0.001, 0.01),\n",
    "    regularizations=(1e-5, 1e-4, 1e-3, 1e-2),\n",
    "    iterations_grid=(40, 60, 80),\n",
    "    k_eval=3,\n",
    "    objective_weights=(0.60, 0.30, 0.10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "03c39591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30243, 9)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"df_validation\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3597c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Access what you need for model evaluation:\n",
    "best_cfg = results[\"best\"][\"cfg\"]\n",
    "best_metrics = results[\"best\"][\"metrics\"]\n",
    "best_model = results[\"best\"][\"model\"]  # tuned on TRAIN\n",
    "final_model = results[\"final_model\"]   # refit on ALL positives\n",
    "UI_all = results[\"UI_all\"]\n",
    "IU_all = results[\"IU_all\"]\n",
    "user_index_all = results[\"user_index_all\"]\n",
    "item_index_all = results[\"item_index_all\"]\n",
    "iid_to_item = results[\"iid_to_item\"]\n",
    "df_validation = results[\"df_validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1203c2",
   "metadata": {},
   "source": [
    "# MODEL EVALUATION ! # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7f1cb27c-902f-46ed-971b-6a4d41422a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_item_from_string(s):\n",
    "    \"\"\"\n",
    "    Convert matchup text to directional 'LEFT ## RIGHT' (preserve order).\n",
    "    Examples:\n",
    "      'Team A @ Team B'  -> 'Team A ## Team B'\n",
    "      'Team A at Team B' -> 'Team A ## Team B'\n",
    "      'Team A vs Team B' -> 'Team A ## Team B'\n",
    "      'Team A & Team B'  -> 'Team A ## Team B'\n",
    "    Returns None if it can't parse exactly two sides.\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "    if isinstance(s, float) and math.isnan(s):\n",
    "        return None\n",
    "\n",
    "    text = str(s).strip()\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    parts = _SEP.split(text)\n",
    "    parts = [p.strip() for p in parts if p.strip()]\n",
    "    if len(parts) != 2:\n",
    "        return None\n",
    "\n",
    "    return parts[0] + \" ## \" + parts[1]\n",
    "                \n",
    "def build_popularity_prior_from_iids(i_all, num_items):\n",
    "    \"\"\"Compute popularity scores as item frequencies normalized by the maximum (0–1) and return both scores and descending rank order.\"\"\"\n",
    "    counts = np.bincount(i_all, minlength=num_items).astype(np.float32)\n",
    "    scores = (counts / counts.max()) if counts.max() > 0 else np.zeros(num_items, dtype=np.float32)\n",
    "    order = np.argsort(-scores)\n",
    "    return scores, order\n",
    "\n",
    "def to_date(s):\n",
    "    return pd.to_datetime(s, errors=\"coerce\").dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "aaf6eae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial num of users in val 439\n",
      "[INFO] Derived i_all from df_train: n=204392\n",
      "[INFO] Users to score: 439 | warm=439 cold=0\n"
     ]
    }
   ],
   "source": [
    "# Types for clarity\n",
    "MaskId = int | str\n",
    "ItemStr = str\n",
    "UID = int\n",
    "IID = int\n",
    "\n",
    "\n",
    "def load_schedules_from_sources(\n",
    "    box_scores_xlsx: str,\n",
    "    box_scores_sheet: str,\n",
    "    vendor_csv: str,\n",
    "    directional_item_from_string: Callable[[str], Optional[str]],\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict[pd.Timestamp, Set[ItemStr]], Dict[pd.Timestamp, Set[ItemStr]]]:\n",
    "    \"\"\"\n",
    "    Load and normalize full schedule (box scores XLSX) and vendor schedule (CSV) into\n",
    "    canonical (date, item) rows where `item` is a directional matchup like \"AWAY ## HOME\",\n",
    "    then build fast per-day lookup sets for each source.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    full_sched : DataFrame\n",
    "        Columns: [\"date\", \"item\"] deduplicated from box scores (dates parsed as MM/DD/YYYY).\n",
    "    vendor_sched : DataFrame\n",
    "        Columns: [\"date\", \"item\"] deduplicated from vendor file (dates parsed as DD-MM-YYYY).\n",
    "    full_by_day : dict[date -> set[item]]\n",
    "        Map of unique items per date from full_sched.\n",
    "    vendor_by_day : dict[date -> set[item]]\n",
    "        Map of unique items per date from vendor_sched.\n",
    "    \"\"\"\n",
    "    # Full slate (Box Scores)\n",
    "    box = pd.read_excel(box_scores_xlsx, sheet_name=box_scores_sheet)\n",
    "    box[\"date\"] = pd.to_datetime(box[\"Game Date\"], format=\"%m/%d/%Y\", errors=\"coerce\").dt.date\n",
    "    box[\"item\"] = box[\"Modified Game Name\"].apply(directional_item_from_string)\n",
    "    full_sched = (\n",
    "        box[[\"date\", \"item\"]]\n",
    "        .dropna()\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Vendor slate\n",
    "    vend = pd.read_csv(vendor_csv)\n",
    "    vend[\"date\"] = pd.to_datetime(\n",
    "        vend[\"Date\"].astype(str).str.strip(), format=\"%d-%m-%Y\", errors=\"coerce\"\n",
    "    ).dt.date\n",
    "    vend[\"item\"] = vend[\"Game\"].apply(directional_item_from_string)\n",
    "    vendor_sched = (\n",
    "        vend[[\"date\", \"item\"]]\n",
    "        .dropna()\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Per-day lookup sets\n",
    "    full_by_day = full_sched.groupby(\"date\")[\"item\"].apply(lambda s: set(s.unique())).to_dict()\n",
    "    vendor_by_day = vendor_sched.groupby(\"date\")[\"item\"].apply(lambda s: set(s.unique())).to_dict()\n",
    "\n",
    "    return full_sched, vendor_sched, full_by_day, vendor_by_day\n",
    "\n",
    "\n",
    "def prepare_validation_truth(\n",
    "    df_validation: pd.DataFrame,\n",
    "    date_start: str | pd.Timestamp,\n",
    "    date_end: str | pd.Timestamp,\n",
    "    directional_item_from_string: Callable[[str], Optional[str]],\n",
    ") -> Tuple[pd.DataFrame, Dict[Tuple[MaskId, pd.Timestamp], Set[ItemStr]], List[MaskId]]:\n",
    "    \"\"\"\n",
    "    Build a clean validation table limited to [date_start, date_end], derive canonical\n",
    "    directional items from event text, and produce ground-truth sets per (mask_id, date)\n",
    "    along with the list of users to score.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    val : DataFrame\n",
    "        Filtered validation rows with columns at least [\"mask_id\",\"date\",\"item\"].\n",
    "    truth_by_ud : dict[(mask_id,date) -> set[item]]\n",
    "        Ground-truth unique items per user-day.\n",
    "    users_to_score : list[mask_id]\n",
    "        Sorted list of unique users present in `val`.\n",
    "    \"\"\"\n",
    "    val = df_validation.copy()\n",
    "    # Scheduled Game Date -> date\n",
    "    val[\"date\"] = pd.to_datetime(val[\"Scheduled Game Date\"], errors=\"coerce\").dt.date\n",
    "    # Directional item from event_description\n",
    "    val[\"item\"] = val[\"event_description\"].apply(directional_item_from_string)\n",
    "    val = val.dropna(subset=[\"mask_id\", \"date\", \"item\"]).copy()\n",
    "\n",
    "    start_d = pd.to_datetime(date_start).date()\n",
    "    end_d = pd.to_datetime(date_end).date()\n",
    "    val = val[(val[\"date\"] >= start_d) & (val[\"date\"] <= end_d)].copy()\n",
    "\n",
    "    print(\"Initial num of users in val\", val[\"mask_id\"].nunique())\n",
    "\n",
    "    truth_by_ud = (\n",
    "        val.groupby([\"mask_id\", \"date\"])[\"item\"]\n",
    "        .apply(lambda s: set(s.unique()))\n",
    "        .to_dict()\n",
    "    )\n",
    "    users_to_score = sorted(val[\"mask_id\"].unique().tolist())\n",
    "    return val, truth_by_ud, users_to_score\n",
    "\n",
    "\n",
    "def build_i_all_from_train(\n",
    "    df_train: pd.DataFrame,\n",
    "    item_index_all: Dict[ItemStr, IID],\n",
    "    directional_item_from_string: Callable[[str], Optional[str]],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Construct an item-id array `i_all` for popularity priors using TRAIN ONLY (no leakage):\n",
    "    parse directional items from train event text and map to global item indices, dropping unknowns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    i_all : np.ndarray[int]\n",
    "        Array of item indices seen in training (possibly with repeats), suitable for bincount.\n",
    "    \"\"\"\n",
    "    _tmp = df_train.copy()\n",
    "    _tmp[\"item\"] = _tmp[\"event_description\"].apply(directional_item_from_string)\n",
    "    i_all = _tmp[\"item\"].map(item_index_all).dropna().astype(int).to_numpy()\n",
    "    print(f\"[INFO] Derived i_all from df_train: n={len(i_all)}\")\n",
    "    return i_all\n",
    "\n",
    "\n",
    "def make_id_mappers_and_ui(\n",
    "    final_model,\n",
    "    UI_all,\n",
    "    user_index_all: Dict[MaskId, UID],\n",
    "    item_index_all: Dict[ItemStr, IID],\n",
    "):\n",
    "    \"\"\"\n",
    "    Create safe mappers from external ids to model indices and return a UI matrix trimmed\n",
    "    to the model's user count for use with `model.recommend`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    uid_for : Callable[[MaskId], Optional[int]]\n",
    "        Maps mask_id -> uid if within model range, else None.\n",
    "    iid_for : Callable[[ItemStr], Optional[int]]\n",
    "        Maps item string -> iid if within model item range, else None.\n",
    "    UI_safe : scipy.sparse.csr_matrix\n",
    "        User×Item matrix sliced to `final_model.user_factors.shape[0]` rows.\n",
    "    model_user_count : int\n",
    "        Number of users known by the model.\n",
    "    model_item_count : int\n",
    "        Number of items known by the model.\n",
    "    \"\"\"\n",
    "    model_user_count = getattr(final_model, \"user_factors\", np.empty((0, 0))).shape[0]\n",
    "    model_item_count = getattr(final_model, \"item_factors\", np.empty((0, 0))).shape[0]\n",
    "    UI_safe = UI_all[:model_user_count, :]\n",
    "\n",
    "    def uid_for(mask_id: MaskId) -> Optional[int]:\n",
    "        uid = user_index_all.get(mask_id, None)\n",
    "        if uid is None or uid < 0 or uid >= model_user_count:\n",
    "            return None\n",
    "        return int(uid)\n",
    "\n",
    "    def iid_for(item: ItemStr) -> Optional[int]:\n",
    "        iid = item_index_all.get(item, None)\n",
    "        if iid is None or iid < 0 or iid >= model_item_count:\n",
    "            return None\n",
    "        return int(iid)\n",
    "\n",
    "    return uid_for, iid_for, UI_safe, model_user_count, model_item_count\n",
    "\n",
    "\n",
    "def precompute_user_recommendations(\n",
    "    final_model,\n",
    "    UI_safe,\n",
    "    users_to_score: Iterable[MaskId],\n",
    "    uid_for: Callable[[MaskId], Optional[int]],\n",
    "    items_all: Iterable[ItemStr],\n",
    "    N_req: Optional[int] = None,\n",
    ") -> Tuple[Dict[MaskId, Optional[Dict[int, float]]], int, int]:\n",
    "    \"\"\"\n",
    "    Run `model.recommend` once per warm user to cache recommendation scores, leaving cold\n",
    "    users as None (to be served by popularity later).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    final_model : object\n",
    "        Trained implicit BPR model with `.recommend(uid, UI, N, ...)`.\n",
    "    UI_safe : csr_matrix\n",
    "        User×Item matrix trimmed to the model's user count.\n",
    "    users_to_score : Iterable\n",
    "        External user identifiers (mask_id) to score.\n",
    "    uid_for : Callable\n",
    "        Mapper from mask_id -> uid in model space (or None if out-of-range).\n",
    "    items_all : Iterable\n",
    "        Collection of all item labels (used only to set a reasonable N_req if not provided).\n",
    "    N_req : int, optional\n",
    "        Number of recommendations to request per user; defaults to min(len(items_all), 10000).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    user_rec_map : dict[mask_id -> dict[iid -> score] | None]\n",
    "        Per-user map of model item indices to relevance scores; None for cold users.\n",
    "    n_warm : int\n",
    "        Count of warm users (seen by the model).\n",
    "    n_cold : int\n",
    "        Count of cold users (not seen by the model).\n",
    "    \"\"\"\n",
    "    N_req = min(len(list(items_all)), 10000) if N_req is None else int(N_req)\n",
    "\n",
    "    user_rec_map: Dict[MaskId, Optional[Dict[int, float]]] = {}\n",
    "    n_warm = n_cold = 0\n",
    "\n",
    "    for mask_id in users_to_score:\n",
    "        uid = uid_for(mask_id)\n",
    "        if uid is not None:\n",
    "            rec_i, rec_s = final_model.recommend(\n",
    "                uid,\n",
    "                UI_safe,\n",
    "                N=N_req,\n",
    "                filter_items=None,\n",
    "                filter_already_liked_items=False,\n",
    "            )\n",
    "            rec_dict = {int(i): float(s) for i, s in zip(rec_i.tolist(), rec_s.tolist())}\n",
    "            user_rec_map[mask_id] = rec_dict\n",
    "            n_warm += 1\n",
    "        else:\n",
    "            user_rec_map[mask_id] = None  # cold -> popularity later\n",
    "            n_cold += 1\n",
    "\n",
    "    #print(f\"[INFO] Users to score: {len(list(users_to_score))} | warm={n_warm} cold={n_cold}\")\n",
    "    return user_rec_map, n_warm, n_cold\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Orchestrated convenience\n",
    "# -------------------------\n",
    "def build_scoring_artifacts(\n",
    "    BOX_SCORES_XLSX: str,\n",
    "    BOX_SCORES_SHEET: str,\n",
    "    VENDOR_CSV: str,\n",
    "    df_validation: pd.DataFrame,\n",
    "    df_train: pd.DataFrame,\n",
    "    items_all: Iterable[ItemStr],\n",
    "    user_index_all: Dict[MaskId, UID],\n",
    "    item_index_all: Dict[ItemStr, IID],\n",
    "    UI_all,                         # csr_matrix (user×item) from your trained pipeline\n",
    "    final_model,                    # fitted implicit BPR model on ALL positives\n",
    "    DATE_START: str | pd.Timestamp,\n",
    "    DATE_END: str | pd.Timestamp,\n",
    "    directional_item_from_string: Callable[[str], Optional[str]],\n",
    "    build_popularity_prior_from_iids: Callable[[np.ndarray, int], Tuple[np.ndarray, np.ndarray]],\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    High-level wrapper: load schedules, prepare validation truth,\n",
    "    derive popularity priors from TRAIN, build safe mappers, and precompute per-user recommendations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    artifacts : dict\n",
    "        Dictionary containing:\n",
    "          - \"full_sched\", \"vendor_sched\", \"full_by_day\", \"vendor_by_day\"\n",
    "          - \"val\", \"truth_by_ud\", \"users_to_score\"\n",
    "          - \"i_all\", \"pop_scores\", \"pop_order\"\n",
    "          - \"uid_for\", \"iid_for\", \"UI_safe\", \"model_user_count\", \"model_item_count\"\n",
    "          - \"user_rec_map\", \"n_warm\", \"n_cold\"\n",
    "    \"\"\"\n",
    "    # 1) Schedules\n",
    "    full_sched, vendor_sched, full_by_day, vendor_by_day = load_schedules_from_sources(\n",
    "        BOX_SCORES_XLSX, BOX_SCORES_SHEET, VENDOR_CSV, directional_item_from_string\n",
    "    )\n",
    "\n",
    "    # 2) Validation truth\n",
    "    val, truth_by_ud, users_to_score = prepare_validation_truth(\n",
    "        df_validation, DATE_START, DATE_END, directional_item_from_string\n",
    "    )\n",
    "\n",
    "    # 3) Popularity prior i_all from TRAIN (no leakage)\n",
    "    i_all = build_i_all_from_train(df_train, item_index_all, directional_item_from_string)\n",
    "\n",
    "    # 4) Priors (scores 0..1 + descending order)\n",
    "    pop_scores, pop_order = build_popularity_prior_from_iids(i_all, num_items=len(list(items_all)))\n",
    "\n",
    "    # 5) Mappers and safe UI\n",
    "    uid_for, iid_for, UI_safe, model_user_count, model_item_count = make_id_mappers_and_ui(\n",
    "        final_model, UI_all, user_index_all, item_index_all\n",
    "    )\n",
    "\n",
    "    # 6) Precompute recommendations per user\n",
    "    user_rec_map, n_warm, n_cold = precompute_user_recommendations(\n",
    "        final_model=final_model,\n",
    "        UI_safe=UI_safe,\n",
    "        users_to_score=users_to_score,\n",
    "        uid_for=uid_for,\n",
    "        items_all=items_all,\n",
    "        N_req=min(len(list(items_all)), 10000),\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        # schedules\n",
    "        \"full_sched\": full_sched,\n",
    "        \"vendor_sched\": vendor_sched,\n",
    "        \"full_by_day\": full_by_day,\n",
    "        \"vendor_by_day\": vendor_by_day,\n",
    "        # validation truth\n",
    "        \"val\": val,\n",
    "        \"truth_by_ud\": truth_by_ud,\n",
    "        \"users_to_score\": users_to_score,\n",
    "        # priors\n",
    "        \"i_all\": i_all,\n",
    "        \"pop_scores\": pop_scores,\n",
    "        \"pop_order\": pop_order,\n",
    "        # mappers and matrices\n",
    "        \"uid_for\": uid_for,\n",
    "        \"iid_for\": iid_for,\n",
    "        \"UI_safe\": UI_safe,\n",
    "        \"model_user_count\": model_user_count,\n",
    "        \"model_item_count\": model_item_count,\n",
    "        # precomputed recs\n",
    "        \"user_rec_map\": user_rec_map,\n",
    "        \"n_warm\": n_warm,\n",
    "        \"n_cold\": n_cold,\n",
    "    }\n",
    "\n",
    "\n",
    "artifacts = build_scoring_artifacts(\n",
    "    BOX_SCORES_XLSX=BOX_SCORES_XLSX,\n",
    "    BOX_SCORES_SHEET=BOX_SCORES_SHEET,\n",
    "    VENDOR_CSV=VENDOR_CSV,\n",
    "    df_validation=df_validation,\n",
    "    df_train=df_train,\n",
    "    items_all=items_all,\n",
    "    user_index_all=user_index_all,\n",
    "    item_index_all=item_index_all,\n",
    "    UI_all=UI_all,\n",
    "    final_model=final_model,\n",
    "    DATE_START=DATE_START,\n",
    "    DATE_END=DATE_END,\n",
    "    directional_item_from_string=directional_item_from_string,\n",
    "    build_popularity_prior_from_iids=build_popularity_prior_from_iids,\n",
    ")\n",
    "\n",
    "\n",
    "# # Reuse anywhere in your workflow:\n",
    "full_by_day = artifacts[\"full_by_day\"]\n",
    "vendor_by_day = artifacts[\"vendor_by_day\"]\n",
    "truth_by_ud = artifacts[\"truth_by_ud\"]\n",
    "pop_scores, pop_order = artifacts[\"pop_scores\"], artifacts[\"pop_order\"]\n",
    "uid_for = artifacts[\"uid_for\"]\n",
    "user_rec_map = artifacts[\"user_rec_map\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4ab2f1c7-0ab4-41e7-b6e3-8080a0406e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_week_of_month(dts: pd.Series) -> pd.Series:\n",
    "    \"\"\"Week-of-month as 1..5 using simple 7-day buckets by day-of-month.\"\"\"\n",
    "    dom = dts.dt.day\n",
    "    return ((dom - 1) // 7 + 1).astype(int)\n",
    "\n",
    "def _infer_date_col(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Try to find a sensible date column in rankings.\n",
    "    Priority order is adjustable.\n",
    "    \"\"\"\n",
    "    candidates = [\"bet_date\", \"date\", \"game_date\", \"event_date\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # fallback: any column with 'date' in its name\n",
    "    for c in df.columns:\n",
    "        if \"date\" in str(c).lower():\n",
    "            return c\n",
    "    raise KeyError(\n",
    "        f\"Could not infer a date column. Looked for any of {candidates} or *date* in {list(df.columns)}.\"\n",
    "    )\n",
    "\n",
    "def augment_rankings_for_export(\n",
    "    rankings: pd.DataFrame,\n",
    "    *,\n",
    "    date_col: str | None = None,     # None => auto-detect\n",
    "    model=None,                      # trained implicit BPR model (optional)\n",
    "    user_index: dict | None = None   # {mask_id -> uid} (optional)\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Modifies a rankings DataFrame for export:\n",
    "      • drops 'score' (if present)\n",
    "      • adds 'new_user' (1=cold, 0=warm)\n",
    "      • adds time columns: 'month', 'week_of_month', 'day_type' derived from the date column\n",
    "\n",
    "    new_user logic (checked in this order):\n",
    "      1) If 'uid' present and `model` provided:\n",
    "         uid >= model.user_factors.shape[0] → cold (1)\n",
    "      2) Else if 'mask_id' present and `user_index` provided:\n",
    "         mask_id not in user_index → cold (1)\n",
    "      3) Else: default warm (0) and print a warning.\n",
    "    \"\"\"\n",
    "    df = rankings.copy()\n",
    "\n",
    "    # Drop score safely\n",
    "    df = df.drop(columns=[\"score\"], errors=\"ignore\")\n",
    "\n",
    "    # new_user\n",
    "    # If you've already computed df[\"new_user\"] upstream, KEEP it and don't overwrite.\n",
    "    if \"new_user\" not in df.columns:\n",
    "        new_user = pd.Series(0, index=df.index, dtype=np.int8)\n",
    "        try:\n",
    "            if \"uid\" in df.columns and model is not None:\n",
    "                n_model_users = int(getattr(model, \"user_factors\", np.empty((0, 0))).shape[0])\n",
    "                uid_vals = pd.to_numeric(df[\"uid\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "                new_user = ((uid_vals < 0) | (uid_vals >= n_model_users)).astype(np.int8)\n",
    "            elif \"mask_id\" in df.columns and user_index is not None:\n",
    "                known = set(user_index.keys())\n",
    "                new_user = (~df[\"mask_id\"].isin(known)).astype(np.int8)\n",
    "            else:\n",
    "                #print(\"[WARN] Could not determine warm/cold users; 'new_user' defaulted to 0 for all rows.\")\n",
    "        except Exception as e:\n",
    "            #print(f\"[WARN] Failed to compute 'new_user' ({e}); defaulting to 0.\")\n",
    "        df[\"new_user\"] = new_user\n",
    "    # else: leave the existing df[\"new_user\"] as-is\n",
    "\n",
    "\n",
    "\n",
    "    # Time features\n",
    "    if date_col is None:\n",
    "        date_col = _infer_date_col(df)\n",
    "\n",
    "    dts = pd.to_datetime(df[date_col], errors=\"coerce\", utc=False)\n",
    "    if dts.isna().any():\n",
    "        bad = df.loc[dts.isna(), date_col].head(3).tolist()\n",
    "        raise ValueError(f\"Some '{date_col}' values could not be parsed as datetime. Examples: {bad}\")\n",
    "\n",
    "    df[\"month\"] = dts.dt.month.astype(np.int8)                # 1..12\n",
    "    df[\"week_of_month\"] = _compute_week_of_month(dts)         # 1..5\n",
    "    df[\"day_type\"] = np.where(dts.dt.dayofweek >= 5, \"Weekend\", \"Weekday\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "dffc6035-49af-4e8a-ac43-d927d830fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ BUILD RANKINGS (BOTH TRACKS) ============\n",
    "\n",
    "def both_orientations(vendor_item: str) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Expand a vendor matchup string into both possible orientations.\n",
    "    Examples\n",
    "    --------\n",
    "    - \"LAL ## BOS\" → {\"LAL ## BOS\", \"BOS ## LAL\"}\n",
    "    - \"LAL @ BOS\"  → {\"LAL @ BOS\"}  (unchanged if no \"##\")\n",
    "    Parameters\n",
    "    ----------\n",
    "    vendor_item : str\n",
    "        Vendor-supplied matchup string, possibly using \"##\" to separate teams.\n",
    "    Returns\n",
    "    -------\n",
    "    set of str\n",
    "        One or two canonical item strings representing both home/away orientations.\n",
    "    \"\"\"\n",
    "    if \"##\" not in vendor_item:\n",
    "        return {vendor_item}\n",
    "    a, b = [p.strip() for p in vendor_item.split(\"##\", 1)]\n",
    "    return {f\"{a} ## {b}\", f\"{b} ## {a}\"}\n",
    "\n",
    "def build_daily_slates(\n",
    "    d,full_by_day: Dict,vendor_by_day: Dict,item_to_iid: Dict[str, int],unknown_full_log: Dict,unknown_vendor_log: Dict,) -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Build the set of known items (mapped to iids) for a given date from both the official (full) slate and the vendor slate.\n",
    "\n",
    "    Behavior\n",
    "    --------\n",
    "    - Full slate: uses directional keys like \"A @ B\" directly; logs unknowns.\n",
    "    - Vendor slate: expands \"A ## B\" into both orientations, deduplicates, and maps to iids; logs unknowns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : date\n",
    "        Target game date.\n",
    "    full_by_day : dict[date -> set[str]]\n",
    "        Mapping from date to full schedule items in \"Away @ Home\" format.\n",
    "    vendor_by_day : dict[date -> set[str]]\n",
    "        Mapping from date to vendor schedule items in \"A ## B\" format.\n",
    "    item_to_iid : dict[str, int]\n",
    "        Dictionary mapping canonical item strings to integer ids.\n",
    "    unknown_full_log : dict\n",
    "        Updated in-place with counts of full slate items missing from catalog.\n",
    "    unknown_vendor_log : dict\n",
    "        Updated in-place with counts of vendor slate items missing from catalog.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    full_iids : list[int]\n",
    "        List of iids from the full slate for date d.\n",
    "    vendor_iids : list[int]\n",
    "        List of iids from the vendor slate (both orientations) for date d.\n",
    "    \"\"\"\n",
    "    all_known = set(item_to_iid.keys())\n",
    "\n",
    "    # --- FULL track (directional, as-is) ---\n",
    "    full_items = full_by_day.get(d, set())\n",
    "    #print(\"Full items\",full_items)\n",
    "    # unknowns (directional keys not found in catalog)\n",
    "    unk_full = [it for it in full_items if it not in all_known]\n",
    "    if unk_full:\n",
    "        unknown_full_log[d] = len(unk_full)\n",
    "    full_iids = [item_to_iid[it] for it in full_items if it in item_to_iid]\n",
    "\n",
    "    # --- VENDOR track (expand, then map) ---\n",
    "    vendor_items = vendor_by_day.get(d, set())\n",
    "    vendor_items_expanded: Set[str] = set()\n",
    "    for it in vendor_items:\n",
    "        vendor_items_expanded |= both_orientations(it)\n",
    "\n",
    "    #print(\"Vendor items:\",vendor_items_expanded)\n",
    "    vendor_iids = [item_to_iid[it] for it in vendor_items_expanded if it in item_to_iid]\n",
    "    \n",
    "    # unknowns after expansion (i.e., 'A @ B'/'B @ A' not in catalog)\n",
    "    unk_vendor = [it for it in vendor_items_expanded if it not in all_known]\n",
    "    if unk_vendor:\n",
    "        unknown_vendor_log[d] = len(unk_vendor)\n",
    "\n",
    "    vendor_iids = list({item_to_iid[it] for it in vendor_items_expanded if it in item_to_iid})\n",
    "    return full_iids, vendor_iids\n",
    "\n",
    "def build_score_map(union_iids: Iterable[int],rec_map,pop_scores: Dict[int, float], iid_to_index: Optional[Dict[int, int]] = None) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Construct a consistent {iid -> score} mapping from various recommendation formats.\n",
    "\n",
    "    Resolution order\n",
    "    ----------------\n",
    "    - If `rec_map` is a dict: use rec_map[iid], else back off to popularity.\n",
    "    - If `rec_map` is a pandas Series: align by iid index, missing values fall back to popularity.\n",
    "    - If `rec_map` is a NumPy array:\n",
    "        * If `iid_to_index` is provided, map iids to array positions.\n",
    "        * Else assume direct indexing (iids 0..n-1); out-of-range values fall back to popularity.\n",
    "    - Any other type: assign popularity scores only.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    union_iids : iterable of int\n",
    "        Item ids that need scores.\n",
    "    rec_map : dict, pandas.Series, or numpy.ndarray\n",
    "        Recommendation scores in one of the supported formats.\n",
    "    pop_scores : dict[int, float]\n",
    "        Popularity-based fallback scores (0–1).\n",
    "    iid_to_index : dict[int, int], optional\n",
    "        Mapping from iid -> position in rec_map if rec_map is an ndarray.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scores : dict[int, float]\n",
    "        Dictionary of iid -> score for all requested items.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "\n",
    "    # Case 1: dict-like (fast path)\n",
    "    if isinstance(rec_map, dict):\n",
    "        for iid in union_iids:\n",
    "            scores[iid] = float(rec_map.get(iid, pop_scores.get(iid, 0.0)))\n",
    "        return scores\n",
    "\n",
    "    # Case 2: pandas Series (index are iids)\n",
    "    if isinstance(rec_map, pd.Series):\n",
    "        # Pull only needed iids; unseen -> NaN -> backoff to pop\n",
    "        sub = rec_map.reindex(list(union_iids))\n",
    "        for iid, val in sub.items():\n",
    "            if pd.isna(val):\n",
    "                scores[iid] = float(pop_scores.get(iid, 0.0))\n",
    "            else:\n",
    "                scores[iid] = float(val)\n",
    "        return scores\n",
    "\n",
    "    # Case 3: numpy array\n",
    "    if isinstance(rec_map, np.ndarray):\n",
    "        n = len(rec_map)\n",
    "        for iid in union_iids:\n",
    "            if iid_to_index is not None:\n",
    "                pos = iid_to_index.get(iid)\n",
    "                if pos is not None and 0 <= pos < n:\n",
    "                    scores[iid] = float(rec_map[pos])\n",
    "                else:\n",
    "                    scores[iid] = float(pop_scores.get(iid, 0.0))\n",
    "            else:\n",
    "                # Assume iids are 0..n-1\n",
    "                if 0 <= iid < n:\n",
    "                    scores[iid] = float(rec_map[iid])\n",
    "                else:\n",
    "                    scores[iid] = float(pop_scores.get(iid, 0.0))\n",
    "        return scores\n",
    "\n",
    "    # Unknown type: fall back entirely to popularity\n",
    "    for iid in union_iids:\n",
    "        scores[iid] = float(pop_scores.get(iid, 0.0))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a247d00d-1053-4be8-9fd9-cf2da94dde55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(rows) = 28974\n",
      "users_to_score size = 439\n",
      "any full_iids ever? True\n",
      "any vendor_iids ever? True\n",
      "[INFO] Saved rankings: ./rankings_may01_14.csv | rows=28974\n"
     ]
    }
   ],
   "source": [
    "def score_iid(iid: int, rec_map: Optional[Dict[int, float]], pop_scores: Dict[int, float]) -> float:\n",
    "    \"\"\"\n",
    "    Return a numeric score for an item id `iid`, preferring the user's recommendation map if available, otherwise falling back to popularity.\n",
    "    \"\"\"\n",
    "    if rec_map is None:\n",
    "        return float(pop_scores.get(iid, 0.0))\n",
    "    try:\n",
    "        return float(rec_map[iid])\n",
    "    except (IndexError, TypeError, KeyError):\n",
    "        return float(pop_scores.get(iid, 0.0))\n",
    "\n",
    "def build_calibration_pairs_topn(\n",
    "    dates_range: Iterable[pd.Timestamp],\n",
    "    users_to_score: Iterable,\n",
    "    full_by_day: Dict,              # date -> set[str] like {'LAL @ BOS', ...}\n",
    "    vendor_by_day: Dict,            # date -> set[str] like {'BOS ## LAL', ...}\n",
    "    item_to_iid: Dict[str, int],\n",
    "    user_rec_map: Dict,             # mask_id -> dict[iid->score] or None\n",
    "    pop_scores: Dict[int, float],\n",
    "    truth_by_ud: Dict[Tuple, Set],  # (mask_id,date) -> set of truth items (iid or item str)\n",
    "    iid_to_item: Dict[int, str],\n",
    "    build_daily_slates: Callable[..., Tuple[List[int], List[int]]],\n",
    "    unknown_full_log: Dict,         # updated in-place\n",
    "    unknown_vendor_log: Dict,       # updated in-place\n",
    "    top_n: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Construct (scores, labels) used to train a confidence calibrator by taking the top-N scored items\n",
    "    per (user, day) on the union of full and vendor slates and labeling them by ground-truth membership.\n",
    "    \"\"\"\n",
    "    scores: List[float] = []\n",
    "    labels: List[int] = []\n",
    "\n",
    "    for d in dates_range:\n",
    "        full_iids, vendor_iids = build_daily_slates(\n",
    "            d, full_by_day, vendor_by_day, item_to_iid, unknown_full_log, unknown_vendor_log\n",
    "        )\n",
    "        if not full_iids and not vendor_iids:\n",
    "            continue\n",
    "\n",
    "        union_iids = set(full_iids) | set(vendor_iids)\n",
    "\n",
    "        for mask_id in users_to_score:\n",
    "            rec_map = user_rec_map.get(mask_id)\n",
    "            truth = truth_by_ud.get((mask_id, d), set())\n",
    "            # Truth may be a set of iids or item strings; detect and compare accordingly.\n",
    "            truth_is_iid = len(truth) > 0 and isinstance(next(iter(truth)), (int, np.integer))\n",
    "\n",
    "            scored = [(iid, score_iid(iid, rec_map, pop_scores)) for iid in union_iids]\n",
    "            scored.sort(key=lambda t: -t[1])\n",
    "\n",
    "            for iid, s in scored[:top_n]:\n",
    "                is_hit = (iid in truth) if truth_is_iid else (iid_to_item.get(iid) in truth)\n",
    "                scores.append(float(s))\n",
    "                labels.append(1 if is_hit else 0)\n",
    "\n",
    "    return np.asarray(scores, dtype=float), np.asarray(labels, dtype=int)\n",
    "\n",
    "\n",
    "def fit_platt_calibrator(val_scores: np.ndarray, val_labels: np.ndarray) -> LogisticRegression:\n",
    "    \"\"\"\n",
    "    Fit a Platt-scaling logistic regression calibrator P(y=1 | score) from (score, label) pairs, with a safe fallback if labels are degenerate.\n",
    "    \"\"\"\n",
    "    if val_scores.size == 0 or len(np.unique(val_labels)) < 2:\n",
    "        # Fallback: approximate identity mapping (monotone) using two anchor points.\n",
    "        platt = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "        platt.fit(np.array([[0.0], [1.0]]), np.array([0, 1]))\n",
    "        return platt\n",
    "\n",
    "    platt = LogisticRegression(\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",  # mitigate class imbalance\n",
    "    )\n",
    "    platt.fit(val_scores.reshape(-1, 1), val_labels)\n",
    "    return platt\n",
    "\n",
    "\n",
    "def generate_rankings_with_calibrated_confidence(\n",
    "    dates_range: Iterable[pd.Timestamp],\n",
    "    users_to_score: Iterable,\n",
    "    full_by_day: Dict,\n",
    "    vendor_by_day: Dict,\n",
    "    item_to_iid: Dict[str, int],\n",
    "    user_rec_map: Dict,                      # mask_id -> dict[iid->score] or None\n",
    "    pop_scores: Dict[int, float],\n",
    "    iid_to_item: Dict[int, str],\n",
    "    platt: LogisticRegression,\n",
    "    build_daily_slates: Callable[..., Tuple[List[int], List[int]]],\n",
    "    unknown_full_log: Dict,\n",
    "    unknown_vendor_log: Dict,\n",
    "    top_k: int,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Produce per-user, per-day rankings for both the full and vendor tracks with calibrated confidence scores using a fitted Platt model.\n",
    "    \"\"\"\n",
    "    rows: List[Dict] = []\n",
    "\n",
    "    for d in dates_range:\n",
    "        full_iids, vendor_iids = build_daily_slates(\n",
    "            d, full_by_day, vendor_by_day, item_to_iid, unknown_full_log, unknown_vendor_log\n",
    "        )\n",
    "        if not full_iids and not vendor_iids:\n",
    "            continue\n",
    "\n",
    "        # Score the union once so both tracks reuse the same scores.\n",
    "        union_iids = set(full_iids) | set(vendor_iids)\n",
    "\n",
    "        for mask_id in users_to_score:\n",
    "            rec_map = user_rec_map.get(mask_id)  # may be None (cold)\n",
    "            score_map = {iid: score_iid(iid, rec_map, pop_scores) for iid in union_iids}\n",
    "\n",
    "            # FULL track\n",
    "            full_ranked = sorted(((iid, score_map[iid]) for iid in full_iids), key=lambda t: -t[1])\n",
    "            full_top = full_ranked[:top_k]\n",
    "            if len(full_top) > 0:\n",
    "                slate_scores = np.array([s for _, s in full_top], dtype=float)\n",
    "                conf_full = platt.predict_proba(slate_scores.reshape(-1, 1))[:, 1]\n",
    "            else:\n",
    "                conf_full = np.array([], dtype=float)\n",
    "\n",
    "            for rank, ((iid, score), conf) in enumerate(zip(full_top, conf_full), 1):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"date\": d,\n",
    "                        \"mask_id\": mask_id,\n",
    "                        \"iid\": iid,\n",
    "                        \"item\": iid_to_item[iid],\n",
    "                        \"score\": float(score),\n",
    "                        \"confidence\": float(conf),\n",
    "                        \"rank\": rank,\n",
    "                        \"track\": \"full\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # VENDOR track\n",
    "            vendor_ranked = sorted(((iid, score_map[iid]) for iid in vendor_iids), key=lambda t: -t[1])\n",
    "            vendor_top = vendor_ranked[:top_k]\n",
    "            if len(vendor_top) > 0:\n",
    "                slate_scores = np.array([s for _, s in vendor_top], dtype=float)\n",
    "                conf_vendor = platt.predict_proba(slate_scores.reshape(-1, 1))[:, 1]\n",
    "            else:\n",
    "                conf_vendor = np.array([], dtype=float)\n",
    "\n",
    "            for rank, ((iid, score), conf) in enumerate(zip(vendor_top, conf_vendor), 1):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"date\": d,\n",
    "                        \"mask_id\": mask_id,\n",
    "                        \"iid\": iid,\n",
    "                        \"item\": iid_to_item[iid],\n",
    "                        \"score\": float(score),\n",
    "                        \"confidence\": float(conf),\n",
    "                        \"rank\": rank,\n",
    "                        \"track\": \"vendor\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "def run_calibrated_scoring_and_export(\n",
    "    *,\n",
    "    # Inputs carried from previous blocks\n",
    "    users_to_score: Iterable,\n",
    "    user_rec_map: Dict[object, Optional[Dict[int, float]]],\n",
    "    pop_scores: Dict[int, float],\n",
    "    iid_to_item: Dict[int, str],\n",
    "    uid_for: Callable[[object], Optional[int]],\n",
    "    full_by_day: Dict,\n",
    "    vendor_by_day: Dict,\n",
    "    item_to_iid: Dict[str, int],\n",
    "    truth_by_ud: Dict[Tuple, Set],\n",
    "    # Config\n",
    "    DATE_START: str | pd.Timestamp,\n",
    "    DATE_END: str | pd.Timestamp,\n",
    "    TOP_K: int,\n",
    "    TOP_N_CAL: int,\n",
    "    # Utilities\n",
    "    build_daily_slates: Callable[..., Tuple[List[int], List[int]]],\n",
    "    augment_rankings_for_export: Callable[..., pd.DataFrame],\n",
    "    OUTPUT_CSV: str,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Orchestrate calibrated ranking generation: build calibration pairs, fit Platt scaler, score both tracks with calibrated confidence, augment results, and save to CSV.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    artifacts : dict\n",
    "        {\n",
    "          \"unknown_full_log\": defaultdict(int),\n",
    "          \"unknown_vendor_log\": defaultdict(int),\n",
    "          \"dates_range\": np.ndarray of dates,\n",
    "          \"val_scores\": np.ndarray,\n",
    "          \"val_labels\": np.ndarray,\n",
    "          \"platt\": LogisticRegression,\n",
    "          \"rows\": list[dict],\n",
    "          \"df_all\": pd.DataFrame,\n",
    "          \"df_all_aug\": pd.DataFrame,\n",
    "          \"output_csv\": str\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Coverage diagnostics\n",
    "    unknown_full_log: Dict = defaultdict(int)\n",
    "    unknown_vendor_log: Dict = defaultdict(int)\n",
    "\n",
    "    # Date range [inclusive]\n",
    "    dates_range = pd.date_range(DATE_START, DATE_END, freq=\"D\").date\n",
    "\n",
    "    # --- Build calibration pairs and fit Platt scaler ---\n",
    "    val_scores, val_labels = build_calibration_pairs_topn(\n",
    "        dates_range=dates_range,\n",
    "        users_to_score=users_to_score,\n",
    "        full_by_day=full_by_day,\n",
    "        vendor_by_day=vendor_by_day,\n",
    "        item_to_iid=item_to_iid,\n",
    "        user_rec_map=user_rec_map,\n",
    "        pop_scores=pop_scores,\n",
    "        truth_by_ud=truth_by_ud,\n",
    "        iid_to_item=iid_to_item,\n",
    "        build_daily_slates=build_daily_slates,\n",
    "        unknown_full_log=unknown_full_log,\n",
    "        unknown_vendor_log=unknown_vendor_log,\n",
    "        top_n=TOP_N_CAL,\n",
    "    )\n",
    "\n",
    "    platt = fit_platt_calibrator(val_scores, val_labels)\n",
    "\n",
    "    # --- Main evaluation loop with calibrated confidence ---\n",
    "    rows = generate_rankings_with_calibrated_confidence(\n",
    "        dates_range=dates_range,\n",
    "        users_to_score=users_to_score,\n",
    "        full_by_day=full_by_day,\n",
    "        vendor_by_day=vendor_by_day,\n",
    "        item_to_iid=item_to_iid,\n",
    "        user_rec_map=user_rec_map,\n",
    "        pop_scores=pop_scores,\n",
    "        iid_to_item=iid_to_item,\n",
    "        platt=platt,\n",
    "        build_daily_slates=build_daily_slates,\n",
    "        unknown_full_log=unknown_full_log,\n",
    "        unknown_vendor_log=unknown_vendor_log,\n",
    "        top_k=TOP_K,\n",
    "    )\n",
    "\n",
    "    print(\"len(rows) =\", len(rows))\n",
    "    print(\"users_to_score size =\", len(list(users_to_score)))\n",
    "    print(\"any full_iids ever?\", any(full_by_day.get(d, set()) for d in dates_range))\n",
    "    print(\"any vendor_iids ever?\", any(vendor_by_day.get(d, set()) for d in dates_range))\n",
    "\n",
    "    # Build DataFrame\n",
    "    df_all = pd.DataFrame(rows)\n",
    "\n",
    "    # Add new_user flag via model visibility (matches warm/cold log)\n",
    "    df_all[\"new_user\"] = df_all[\"mask_id\"].apply(lambda mid: 1 if uid_for(mid) is None else 0).astype(\"int8\")\n",
    "\n",
    "    # Augment + export\n",
    "    df_all_aug = augment_rankings_for_export(df_all, date_col=None)\n",
    "    Path(OUTPUT_CSV).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_all_aug.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"[INFO] Saved rankings: {OUTPUT_CSV} | rows={len(df_all_aug)}\")\n",
    "\n",
    "    if unknown_full_log or unknown_vendor_log:\n",
    "        miss_full_total = sum(unknown_full_log.values())\n",
    "        miss_vendor_total = sum(unknown_vendor_log.values())\n",
    "        #print(f\"[COVERAGE] Unknown items (not in items_all) -> full_track: {miss_full_total} | vendor_track: {miss_vendor_total}\")\n",
    "\n",
    "    return {\n",
    "        \"unknown_full_log\": unknown_full_log,\n",
    "        \"unknown_vendor_log\": unknown_vendor_log,\n",
    "        \"dates_range\": dates_range,\n",
    "        \"val_scores\": val_scores,\n",
    "        \"val_labels\": val_labels,\n",
    "        \"platt\": platt,\n",
    "        \"rows\": rows,\n",
    "        \"df_all\": df_all,\n",
    "        \"df_all_aug\": df_all_aug,\n",
    "        \"output_csv\": OUTPUT_CSV,\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Example orchestrated call\n",
    "# --------------------------\n",
    "artifacts = run_calibrated_scoring_and_export(\n",
    "    users_to_score=users_to_score,\n",
    "    user_rec_map=user_rec_map,\n",
    "    pop_scores=pop_scores,\n",
    "    iid_to_item=iid_to_item,\n",
    "    uid_for=uid_for,\n",
    "    full_by_day=full_by_day,\n",
    "    vendor_by_day=vendor_by_day,\n",
    "    item_to_iid=item_index_all,  # or item_to_iid if that's your mapping\n",
    "    truth_by_ud=truth_by_ud,\n",
    "    DATE_START=DATE_START,\n",
    "    DATE_END=DATE_END,\n",
    "    TOP_K=3,\n",
    "    TOP_N_CAL=5,\n",
    "    build_daily_slates=build_daily_slates,\n",
    "    augment_rankings_for_export=augment_rankings_for_export,\n",
    "    OUTPUT_CSV=OUTPUT_CSV)\n",
    "df_all_aug = artifacts[\"df_all_aug\"]  # ready for app/export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "05f8df06-92c4-4b10-8375-db45867982dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SEGMENT] All players\n",
      "  (Actual Games Played)\n",
      "    Number of bets = 6146\n",
      "    Precision@3    = 0.2929\n",
      "    NDCG@3         = 0.3348\n",
      "    HitCoverage@3  = 0.3581\n",
      "  (Pre-season schedule)\n",
      "    Number of bets = 6146\n",
      "    Precision@3    = 0.1164\n",
      "    NDCG@3         = 0.1702\n",
      "    HitCoverage@3  = 0.2790\n",
      "\n",
      "[SEGMENT] Weekend games\n",
      "  (Actual Games Played)\n",
      "    Number of bets = 1756\n",
      "    Precision@3    = 0.3118\n",
      "    NDCG@3         = 0.3539\n",
      "    HitCoverage@3  = 0.3770\n",
      "  (Pre-season schedule)\n",
      "    Number of bets = 1756\n",
      "    Precision@3    = 0.1281\n",
      "    NDCG@3         = 0.1920\n",
      "    HitCoverage@3  = 0.2779\n",
      "\n",
      "[SEGMENT] Weekday games\n",
      "  (Actual Games Played)\n",
      "    Number of bets = 4390\n",
      "    Precision@3    = 0.2853\n",
      "    NDCG@3         = 0.3272\n",
      "    HitCoverage@3  = 0.3506\n",
      "  (Pre-season schedule)\n",
      "    Number of bets = 4390\n",
      "    Precision@3    = 0.1118\n",
      "    NDCG@3         = 0.1615\n",
      "    HitCoverage@3  = 0.2795\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Core metric computation\n",
    "# =========================\n",
    "def eval_metrics_at_k(\n",
    "    df_rankings: pd.DataFrame,\n",
    "    truth_by_ud: Dict[Tuple[object, object], set],\n",
    "    k: int = 3,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute Precision@k, NDCG@k, HitCoverage@k over user-day slices in `df_rankings`,\n",
    "    where ground-truth items are provided by `truth_by_ud[(mask_id, date)]`.\n",
    "    \"\"\"\n",
    "    if df_rankings is None or df_rankings.empty:\n",
    "        return {f\"Precision@{k}\": 0.0, f\"NDCG@{k}\": 0.0, f\"HitCoverage@{k}\": 0.0, \"n_eval\": 0}\n",
    "\n",
    "    # Make a safe copy and ensure numeric rank\n",
    "    recs = df_rankings.copy()\n",
    "    recs[\"rank\"] = pd.to_numeric(recs[\"rank\"], errors=\"coerce\")\n",
    "    recs = recs[recs[\"rank\"].notna()]\n",
    "\n",
    "    # Keep only top-k rows\n",
    "    recs = recs[recs[\"rank\"] <= k]\n",
    "    if recs.empty:\n",
    "        return {f\"Precision@{k}\": 0.0, f\"NDCG@{k}\": 0.0, f\"HitCoverage@{k}\": 0.0, \"n_eval\": 0}\n",
    "\n",
    "    total_prec = total_ndcg = total_cov = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for (mask_id, d), grp in recs.groupby([\"mask_id\", \"date\"]):\n",
    "        topk_items = grp.sort_values(\"rank\")[\"item\"].tolist()\n",
    "        if not topk_items:\n",
    "            continue\n",
    "\n",
    "        true_items = truth_by_ud.get((mask_id, d), set())\n",
    "        hits = sum(1 for it in topk_items if it in true_items)\n",
    "\n",
    "        # Precision@k denominator uses the actual number of recs shown (<= k)\n",
    "        denom = min(k, len(topk_items))\n",
    "        prec_k = (hits / denom) if denom > 0 else 0.0\n",
    "\n",
    "        # NDCG@k\n",
    "        ideal = min(len(true_items), len(topk_items), k)\n",
    "        if ideal > 0:\n",
    "            idcg = sum(1.0 / np.log2(r + 1) for r in range(1, ideal + 1))\n",
    "            dcg = sum(1.0 / np.log2(r + 1) for r, it in enumerate(topk_items, 1) if it in true_items)\n",
    "            ndcg = dcg / idcg\n",
    "        else:\n",
    "            ndcg = 0.0\n",
    "\n",
    "        # HitCoverage@k = at least one correct in the slate\n",
    "        cov = 1.0 if hits > 0 else 0.0\n",
    "\n",
    "        total_prec += prec_k\n",
    "        total_ndcg += ndcg\n",
    "        total_cov += cov\n",
    "        n += 1\n",
    "\n",
    "    if n == 0:\n",
    "        return {f\"Precision@{k}\": 0.0, f\"NDCG@{k}\": 0.0, f\"HitCoverage@{k}\": 0.0, \"n_eval\": 0}\n",
    "\n",
    "    return {\n",
    "        f\"Precision@{k}\": total_prec / n,\n",
    "        f\"NDCG@{k}\": total_ndcg / n,\n",
    "        f\"HitCoverage@{k}\": total_cov / n,\n",
    "        \"n_eval\": n,\n",
    "    }\n",
    "\n",
    "\n",
    "def track_metrics(df_all: pd.DataFrame, truth_by_ud: Dict, k: int = 3) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Evaluate metrics per track (e.g., 'full' vs 'vendor') in `df_all` and return a dict of results keyed by track name.\n",
    "    \"\"\"\n",
    "    out: Dict[str, Dict[str, float]] = {}\n",
    "    for track in sorted(df_all[\"track\"].unique()):\n",
    "        m = eval_metrics_at_k(df_all[df_all[\"track\"] == track], truth_by_ud, k=k)\n",
    "        out[track] = m\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Segmentation utilities\n",
    "# =========================\n",
    "def ensure_day_type(df: pd.DataFrame, date_col: str = \"date\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure a 'day_type' column exists ('Weekend'/'Weekday') based on `date_col`, returning a copy if needed.\n",
    "    \"\"\"\n",
    "    if \"day_type\" in df.columns:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    dts = pd.to_datetime(out[date_col], errors=\"coerce\")\n",
    "    out[\"day_type\"] = np.where(dts.dt.dayofweek >= 5, \"Weekend\", \"Weekday\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_segmented_metrics(\n",
    "    df_all_aug: pd.DataFrame,\n",
    "    truth_by_ud: Dict[Tuple[object, object], set],\n",
    "    user_segments: Optional[Sequence[Tuple[str, Callable[[pd.DataFrame], pd.Series]]]] = None,\n",
    "    k: int = 3,\n",
    ") -> Tuple[pd.DataFrame, List[Dict[str, object]]]:\n",
    "    \"\"\"\n",
    "    Compute metrics for:\n",
    "      • All players (new segment)\n",
    "      • Weekend games / Weekday games (built-in)\n",
    "      • Optional custom segments (if provided)\n",
    "\n",
    "    Returns a tidy DataFrame and the raw rows list (for JSON).\n",
    "    \"\"\"\n",
    "    df_seg = ensure_day_type(df_all_aug)\n",
    "\n",
    "    rows: List[Dict[str, object]] = []\n",
    "\n",
    "    def _append_rows(segment_type: str, seg_key: str, title: str, mask: pd.Series):\n",
    "        df_full = df_seg.loc[mask & (df_seg[\"track\"] == \"full\")]\n",
    "        df_vendor = df_seg.loc[mask & (df_seg[\"track\"] == \"vendor\")]\n",
    "\n",
    "        m_full = eval_metrics_at_k(df_full, truth_by_ud, k=k)\n",
    "        m_vendor = eval_metrics_at_k(df_vendor, truth_by_ud, k=k)\n",
    "\n",
    "        # Console print (optional)\n",
    "        print(f\"\\n[SEGMENT] {title}\")\n",
    "        print(\"  (Actual Games Played)\")\n",
    "        print(f\"    Number of bets = {m_full['n_eval']}\")\n",
    "        print(f\"    Precision@{k}    = {m_full[f'Precision@{k}']:.4f}\")\n",
    "        print(f\"    NDCG@{k}         = {m_full[f'NDCG@{k}']:.4f}\")\n",
    "        print(f\"    HitCoverage@{k}  = {m_full[f'HitCoverage@{k}']:.4f}\")\n",
    "        print(\"  (Pre-season schedule)\")\n",
    "        print(f\"    Number of bets = {m_vendor['n_eval']}\")\n",
    "        print(f\"    Precision@{k}    = {m_vendor[f'Precision@{k}']:.4f}\")\n",
    "        print(f\"    NDCG@{k}         = {m_vendor[f'NDCG@{k}']:.4f}\")\n",
    "        print(f\"    HitCoverage@{k}  = {m_vendor[f'HitCoverage@{k}']:.4f}\")\n",
    "\n",
    "        rows.append({\n",
    "            \"segment_type\": segment_type,\n",
    "            \"segment\": seg_key,\n",
    "            \"segment_label\": title,\n",
    "            \"track\": \"Actual Games Played\",\n",
    "            \"number_of_bets\": m_full[\"n_eval\"],\n",
    "            \"precision@k\": m_full[f\"Precision@{k}\"],\n",
    "            \"ndcg@k\": m_full[f\"NDCG@{k}\"],\n",
    "            \"hitcoverage@k\": m_full[f\"HitCoverage@{k}\"],\n",
    "            \"k\": k,\n",
    "        })\n",
    "        rows.append({\n",
    "            \"segment_type\": segment_type,\n",
    "            \"segment\": seg_key,\n",
    "            \"segment_label\": title,\n",
    "            \"track\": \"Pre-season schedule\",\n",
    "            \"number_of_bets\": m_vendor[\"n_eval\"],\n",
    "            \"precision@k\": m_vendor[f\"Precision@{k}\"],\n",
    "            \"ndcg@k\": m_vendor[f\"NDCG@{k}\"],\n",
    "            \"hitcoverage@k\": m_vendor[f\"HitCoverage@{k}\"],\n",
    "            \"k\": k,\n",
    "        })\n",
    "\n",
    "    # 1) New built-in: All players\n",
    "    mask_all = pd.Series(True, index=df_seg.index)\n",
    "    _append_rows(segment_type=\"user\", seg_key=\"all\", title=\"All players\", mask=mask_all)\n",
    "\n",
    "    # 2) Optional user-provided segments\n",
    "    if user_segments:\n",
    "        for title, mask_fn in user_segments:\n",
    "            mask = mask_fn(df_seg)\n",
    "            _append_rows(segment_type=\"custom\", seg_key=title, title=title, mask=mask)\n",
    "\n",
    "    # 3) Built-in day-type segments\n",
    "    day_segments = [\n",
    "        (\"Weekend games\", \"daytype\", \"weekend\", df_seg[\"day_type\"].eq(\"Weekend\")),\n",
    "        (\"Weekday games\", \"daytype\", \"weekday\", df_seg[\"day_type\"].eq(\"Weekday\")),\n",
    "    ]\n",
    "    for title, seg_type, seg_key, mask in day_segments:\n",
    "        _append_rows(segment_type=seg_type, seg_key=seg_key, title=title, mask=mask)\n",
    "\n",
    "    metrics_df = pd.DataFrame(rows)\n",
    "    return metrics_df, rows\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Orchestrator + I/O\n",
    "# =========================\n",
    "def evaluate_and_save_all_metrics(\n",
    "    df_all: pd.DataFrame,\n",
    "    df_all_aug: pd.DataFrame,\n",
    "    truth_by_ud: Dict[Tuple[object, object], set],\n",
    "    output_csv_base: str,\n",
    "    user_segments: Optional[Sequence[Tuple[str, Callable[[pd.DataFrame], pd.Series]]]] = None,\n",
    "    k: int = 3,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Run track-level and segmented evaluations, print a concise report,\n",
    "    and save segmented results to CSV/JSON files next to `output_csv_base`.\n",
    "    \"\"\"\n",
    "    # Track-level\n",
    "    metrics_by_track = track_metrics(df_all, truth_by_ud, k=k)\n",
    "\n",
    "    # Printing (matches your original style)\n",
    "    for track_label, pretty in (\n",
    "        (\"Full track (Box Scores)\", \"full\"),\n",
    "        (\"Vendor track (Pre-season schedule)\", \"vendor\"),\n",
    "    ):\n",
    "        m = metrics_by_track.get(\n",
    "            pretty,\n",
    "            {\"n_eval\": 0, f\"Precision@{k}\": 0.0, f\"NDCG@{k}\": 0.0, f\"HitCoverage@{k}\": 0.0},\n",
    "        )\n",
    "        #print(f\"\\n[METRICS] {track_label}\")\n",
    "        #print(f\"  n_eval         = {m['n_eval']}\")\n",
    "        #print(f\"  Precision@{k}  = {m[f'Precision@{k}']:.4f}\")\n",
    "        #print(f\"  NDCG@{k}       = {m[f'NDCG@{k}']:.4f}\")\n",
    "        #print(f\"  HitCoverage@{k}= {m[f'HitCoverage@{k}']:.4f}\")\n",
    "\n",
    "    # Segmented (now includes All players + day-type + user segments)\n",
    "    seg_df, seg_rows = compute_segmented_metrics(\n",
    "        df_all_aug=df_all_aug,\n",
    "        truth_by_ud=truth_by_ud,\n",
    "        user_segments=user_segments,\n",
    "        k=k,\n",
    "    )\n",
    "\n",
    "    # Save segmented metrics\n",
    "    metrics_csv = output_csv_base.replace(\".csv\", \"_metrics.csv\")\n",
    "    metrics_json = output_csv_base.replace(\".csv\", \"_metrics.json\")\n",
    "    Path(metrics_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "    #seg_df.to_csv(metrics_csv, index=False)\n",
    "    #metrics_csv_s3 = f\"s3://{olg-reco-outputs}/rankings_may01_14.csv\"\n",
    "    #seg_df.to_csv(metrics_csv_s3, index=False)\n",
    "    bucket = \"olg-reco-outputs\"\n",
    "    key = \"rankings_may01_14_metrics.csv\"  # or f\"{prefix}/rankings_may01_14.csv\"\n",
    "    metrics_csv_s3 = f\"s3://{bucket}/{key}\"\n",
    "    seg_df.to_csv(metrics_csv_s3, index=False)\n",
    "\n",
    "\n",
    "    #print(f\"\\n[INFO] Saved segmented metrics: {metrics_csv} & {metrics_json}\")\n",
    "\n",
    "    # Optional preview\n",
    "    #print(\"\\n[PREVIEW] Rankings head()\")\n",
    "    #print(df_all.head(10).to_string(index=False))\n",
    "\n",
    "    return {\n",
    "        \"k\": k,\n",
    "        \"metrics_by_track\": metrics_by_track,\n",
    "        \"segmented_metrics_df\": seg_df,\n",
    "        \"segmented_metrics_rows\": seg_rows,\n",
    "        \"metrics_csv\": metrics_csv,\n",
    "        \"metrics_json\": metrics_json,\n",
    "    }\n",
    "\n",
    "\n",
    "results = evaluate_and_save_all_metrics(\n",
    "    df_all=df_all,\n",
    "    df_all_aug=df_all_aug,\n",
    "    truth_by_ud=truth_by_ud,\n",
    "    output_csv_base=\"rankings_may01_14.csv\",\n",
    "    user_segments=None,  # or e.g., [(\"New users\", lambda df: df[\"new_user\"] == 1)]\n",
    "    k=3,\n",
    ")\n",
    "seg_df = results[\"segmented_metrics_df\"]\n",
    "\n",
    "\n",
    "# Access artifacts later:\n",
    "track_metrics_dict = results[\"metrics_by_track\"]\n",
    "seg_df = results[\"segmented_metrics_df\"]\n",
    "#print(\"Track metrics:\", track_metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8c1068",
   "metadata": {},
   "source": [
    "## Personalized Messaging ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4703803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# =========================\n",
    "# String Cleaning Helpers\n",
    "# =========================\n",
    "\n",
    "def _clean_spaces_upper(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Collapse multiple spaces into one, trim leading/trailing whitespace, and convert to UPPERCASE.\n",
    "    Useful for robust string matching across different event description formats.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        s.astype(str)\n",
    "         .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "         .str.strip()\n",
    "         .str.upper()\n",
    "    )\n",
    "\n",
    "\n",
    "def item_to_event(item_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert matchup strings of the form 'Team A ## Team B' into normalized 'TEAM A @ TEAM B'.\n",
    "    If a row does not contain '##', the result will be NaN.\n",
    "    \"\"\"\n",
    "    parts = item_series.astype(str).str.split(r\"\\s*##\\s*\", n=1, expand=True)\n",
    "    event = parts[0].str.strip() + \" @ \" + parts[1].str.strip()\n",
    "    return _clean_spaces_upper(event)\n",
    "\n",
    "\n",
    "def normalize_event_desc(event_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Normalize event_description strings that already use '@' format into 'TEAM A @ TEAM B',\n",
    "    ensuring uppercase and consistent spacing around '@'.\n",
    "    \"\"\"\n",
    "    s = _clean_spaces_upper(event_series)\n",
    "    return s.str.replace(r\"\\s*@\\s*\", \" @ \", regex=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Feature Builders\n",
    "# =========================\n",
    "\n",
    "def build_player_game_history(df_all: pd.DataFrame, df_train: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enrich `df_all` with per-player and overall game betting history from `df_train`.\n",
    "\n",
    "    Features added\n",
    "    --------------\n",
    "    - No_Of_Bets_Player_Game_History : Number of bets by this player on this game historically.\n",
    "    - Total_No_Of_Bets_For_Game_History : Total number of bets across all players on this game.\n",
    "    - Total_Amount_History : Total wagered amount across all players on this game.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_all : DataFrame\n",
    "        Rankings or scoring DataFrame to enrich; must contain 'mask_id' and 'item'.\n",
    "    df_train : DataFrame\n",
    "        Training bet history with columns ['mask_id', 'event_description', 'wager_amount'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Copy of df_all with new history features added.\n",
    "    \"\"\"\n",
    "    out = df_all.copy()\n",
    "    train = df_train.copy()\n",
    "\n",
    "    # Normalized event keys for joins\n",
    "    out[\"event_norm\"] = item_to_event(out[\"item\"])\n",
    "    train[\"event_norm\"] = normalize_event_desc(train[\"event_description\"])\n",
    "\n",
    "    # Ensure wager_amount is numeric\n",
    "    train[\"wager_amount\"] = pd.to_numeric(train[\"wager_amount\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # --- 1) Player-specific bet history ---\n",
    "    user_event_counts = (\n",
    "        train.groupby([\"mask_id\", \"event_norm\"])\n",
    "             .size()\n",
    "             .reset_index(name=\"No_Of_Bets_Player_Game_History\")\n",
    "    )\n",
    "    out = out.merge(user_event_counts, how=\"left\", on=[\"mask_id\", \"event_norm\"])\n",
    "    out[\"No_Of_Bets_Player_Game_History\"] = (\n",
    "        out[\"No_Of_Bets_Player_Game_History\"].fillna(0).astype(int)\n",
    "    )\n",
    "\n",
    "    # --- 2) Overall game bet history ---\n",
    "    overall = (\n",
    "        train.groupby(\"event_norm\")\n",
    "             .agg(\n",
    "                 Total_No_Of_Bets_For_Game_History=(\"event_norm\", \"size\"),\n",
    "                 Total_Amount_History=(\"wager_amount\", \"sum\"),\n",
    "             )\n",
    "             .reset_index()\n",
    "    )\n",
    "    out = out.merge(overall, how=\"left\", on=\"event_norm\")\n",
    "    out[\"Total_No_Of_Bets_For_Game_History\"] = (\n",
    "        out[\"Total_No_Of_Bets_For_Game_History\"].fillna(0).astype(int)\n",
    "    )\n",
    "    out[\"Total_Amount_History\"] = out[\"Total_Amount_History\"].fillna(0.0)\n",
    "\n",
    "    return out\n",
    "\n",
    "enriched_df = build_player_game_history(df_all, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b75a82c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Data coercion utilities\n",
    "# =========================\n",
    "\n",
    "def coerce_history_columns(out: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure expected history columns exist in `out` and coerce them to numeric types:\n",
    "    - \"No_Of_Bets_Player_Game_History\" → int (missing/invalid → 0)\n",
    "    - \"Total_Amount_History\"           → float (missing/invalid → 0.0)\n",
    "    Returns a new DataFrame without mutating the input.\n",
    "    \"\"\"\n",
    "    df = out.copy()\n",
    "\n",
    "    df[\"No_Of_Bets_Player_Game_History\"] = pd.to_numeric(\n",
    "        df.get(\"No_Of_Bets_Player_Game_History\", 0), errors=\"coerce\"\n",
    "    ).fillna(0).astype(int)\n",
    "\n",
    "    df[\"Total_Amount_History\"] = pd.to_numeric(\n",
    "        df.get(\"Total_Amount_History\", 0), errors=\"coerce\"\n",
    "    ).fillna(0.0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Messaging helpers\n",
    "# =========================\n",
    "\n",
    "def fmt_currency(x: float) -> str:\n",
    "    \"\"\"\n",
    "    Format a numeric value as a whole-dollar string with thousands separators (e.g., 12345 → '12,345').\n",
    "    \"\"\"\n",
    "    return f\"{x:,.0f}\"\n",
    "\n",
    "\n",
    "def build_message_for_row(\n",
    "    row: pd.Series,\n",
    "    currency_formatter: Callable[[float], str] = fmt_currency,\n",
    "    bet_threshold_repeat: int = 3,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a personalized message for a ranking row based on the user's prior bets and total market activity.\n",
    "    Logic mirrors the original snippet: >3 bets → “again”, 0 bets → “never bet”, else generic social proof.\n",
    "    \"\"\"\n",
    "    n_bets = int(row.get(\"No_Of_Bets_Player_Game_History\", 0))\n",
    "    item = str(row.get(\"item\", \"this matchup\"))\n",
    "    total_amt = currency_formatter(float(row.get(\"Total_Amount_History\", 0.0)))\n",
    "\n",
    "    if n_bets > bet_threshold_repeat:\n",
    "        return (\n",
    "            f\"You’ve placed {n_bets} bets on {item} before — it’s back on the schedule! Time to play again?\"\n",
    "        )\n",
    "    elif n_bets == 0:\n",
    "        return (\n",
    "            f\"You’ve never bet on {item} before, but bettors have already wagered over ${total_amt} on it historically. Wanna play?\"\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            f\"Bettors have wagered over ${total_amt} on {item} historically. It’s a matchup worth betting.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def add_personalized_messaging(\n",
    "    out: pd.DataFrame,\n",
    "    currency_formatter: Callable[[float], str] = fmt_currency,\n",
    "    bet_threshold_repeat: int = 3,\n",
    "    column_name: str = \"messaging\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a copy of `out` with a new column (default 'messaging') containing per-row personalized messages,\n",
    "    after safely coercing required history columns to numeric.\n",
    "    \"\"\"\n",
    "    df = coerce_history_columns(out)\n",
    "    df[column_name] = df.apply(\n",
    "        lambda r: build_message_for_row(\n",
    "            r, currency_formatter=currency_formatter, bet_threshold_repeat=bet_threshold_repeat\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "out = coerce_history_columns(enriched_df)\n",
    "out = add_personalized_messaging(out, column_name=\"messaging\")\n",
    "#out[[\"item\", \"No_Of_Bets_Player_Game_History\", \"Total_Amount_History\", \"messaging\"]].head()\n",
    "bucket = \"olg-reco-outputs\"\n",
    "key = \"rankings_may01_14.csv\"  \n",
    "rankings_csv_s3 = f\"s3://{bucket}/{key}\"\n",
    "out.to_csv(rankings_csv_s3, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9414deb2",
   "metadata": {},
   "source": [
    "### Compare model reco based on pre-season schdeule vs actual games played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3705550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_out(out: pd.DataFrame, TOP_K: int = 3):\n",
    "    \"\"\"\n",
    "    Compare recommendation quality between 'full' and 'vendor' tracks in a\n",
    "    per-user-per-day setting using pandas.\n",
    "\n",
    "    What this computes\n",
    "    ------------------\n",
    "    A) Coverage & Volume\n",
    "       - #pairs with full, #pairs with vendor, union size, intersection size\n",
    "       - Intersection rate = intersection / union\n",
    "       - Average list length per track after dedupe and TOP_K\n",
    "\n",
    "    B) Exact Match Rates (over intersection of pairs)\n",
    "       - Strict exact match: same length AND identical ordered list (up to K)\n",
    "       - Set exact match: same multiset of items, ignoring order (up to K)\n",
    "\n",
    "    D) Confidence Comparison\n",
    "       - Mean/median confidence by track overall\n",
    "       - Mean/median confidence by rank position (1..K)\n",
    "       - Δconfidence (full − vendor) at matched rank positions\n",
    "\n",
    "    E) Discrepancy Report (Top 20 pairs)\n",
    "       - Overlap@K (see definition below)\n",
    "       - Mean/median rank shift for common items\n",
    "       - Items only in full / only in vendor\n",
    "       - Sorted by lowest overlap first, then highest mean rank shift\n",
    "\n",
    "    Overlap@K (exact definition)\n",
    "    ----------------------------\n",
    "    For a given pair (date, mask_id), take the Top-K lists from 'full' and 'vendor'\n",
    "    after deduplication. Convert items to lowercase for matching. Then:\n",
    "        Overlap@K = count of items that appear in BOTH lists.\n",
    "    Range: 0 .. min(len(full_list), len(vendor_list), K).\n",
    "    (Order does not matter for overlap; it’s a simple intersection size.)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # 0) Preprocess\n",
    "    # -------------------------\n",
    "    df = out.copy()\n",
    "\n",
    "    # Coerce types\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.date\n",
    "    df[\"rank\"] = pd.to_numeric(df[\"rank\"], errors=\"coerce\")\n",
    "    df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\")\n",
    "    for col in [\"item\", \"track\", \"day_type\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    # Keep only needed tracks and basic non-nulls\n",
    "    df = df[df[\"track\"].isin([\"full\", \"vendor\"])].dropna(\n",
    "        subset=[\"date\", \"mask_id\", \"track\", \"rank\", \"item\"]\n",
    "    )\n",
    "\n",
    "    # Deduplicate within (date, mask_id, track, item): keep lowest rank\n",
    "    df = df.sort_values([\"date\", \"mask_id\", \"track\", \"item\", \"rank\"])\n",
    "    df = df.drop_duplicates([\"date\", \"mask_id\", \"track\", \"item\"], keep=\"first\")\n",
    "\n",
    "    # Keep TOP_K per (date, mask_id, track)\n",
    "    df = df.sort_values([\"date\", \"mask_id\", \"track\", \"rank\"])\n",
    "    df = df.groupby([\"date\", \"mask_id\", \"track\"]).head(TOP_K)\n",
    "\n",
    "    # Helper: build per-pair recs\n",
    "    def build_recs(frame: pd.DataFrame):\n",
    "        recs = {}\n",
    "        for (d, u, t), grp in frame.groupby([\"date\", \"mask_id\", \"track\"]):\n",
    "            grp = grp.sort_values(\"rank\")\n",
    "            items = list(grp[\"item\"])\n",
    "            items_norm = [str(s).lower() for s in items]\n",
    "            ranks = list(grp[\"rank\"].astype(int))\n",
    "            confs = list(grp[\"confidence\"])\n",
    "            recs.setdefault((d, u), {})[t] = {\n",
    "                \"items\": items,\n",
    "                \"items_norm\": items_norm,\n",
    "                \"ranks\": ranks,\n",
    "                \"conf\": confs,\n",
    "            }\n",
    "        return recs\n",
    "\n",
    "    recs = build_recs(df)\n",
    "\n",
    "    pairs_full   = {p for p, d in recs.items() if \"full\" in d}\n",
    "    pairs_vendor = {p for p, d in recs.items() if \"vendor\" in d}\n",
    "    union_pairs  = pairs_full | pairs_vendor\n",
    "    inter_pairs  = pairs_full & pairs_vendor\n",
    "\n",
    "    # -------------------------\n",
    "    # A) Coverage & Volume\n",
    "    # -------------------------\n",
    "    avg_len_full = np.mean([len(recs[p][\"full\"][\"items\"]) for p in pairs_full]) if pairs_full else np.nan\n",
    "    avg_len_vendor = np.mean([len(recs[p][\"vendor\"][\"items\"]) for p in pairs_vendor]) if pairs_vendor else np.nan\n",
    "\n",
    "    coverage_volume = pd.DataFrame({\n",
    "        \"metric\": [\n",
    "            \"pairs_full\",\n",
    "            \"pairs_vendor\",\n",
    "            \"union_pairs\",\n",
    "            \"intersection_pairs\",\n",
    "            \"intersection_rate\",\n",
    "            f\"avg_list_len_full@{TOP_K}\",\n",
    "            f\"avg_list_len_vendor@{TOP_K}\",\n",
    "        ],\n",
    "        \"value\": [\n",
    "            len(pairs_full),\n",
    "            len(pairs_vendor),\n",
    "            len(union_pairs),\n",
    "            len(inter_pairs),\n",
    "            (len(inter_pairs) / len(union_pairs)) if len(union_pairs) else np.nan,\n",
    "            avg_len_full,\n",
    "            avg_len_vendor,\n",
    "        ],\n",
    "    })\n",
    "\n",
    "    # -------------------------\n",
    "    # B) Exact Match Rates (over intersection)\n",
    "    # -------------------------\n",
    "    strict_matches = 0\n",
    "    set_matches = 0\n",
    "\n",
    "    # Also compute pair-level overlap & rank shift for discrepancies\n",
    "    pair_rows = []\n",
    "\n",
    "    for p in inter_pairs:\n",
    "        f = recs[p][\"full\"][\"items_norm\"]\n",
    "        v = recs[p][\"vendor\"][\"items_norm\"]\n",
    "\n",
    "        # Strict: same length and identical ranked list\n",
    "        if (len(f) == len(v)) and (f == v):\n",
    "            strict_matches += 1\n",
    "\n",
    "        # Set exact: same multiset ignoring order\n",
    "        if Counter(f) == Counter(v):\n",
    "            set_matches += 1\n",
    "\n",
    "        # Overlap\n",
    "        sf, sv = set(f), set(v)\n",
    "        overlap = len(sf & sv)\n",
    "\n",
    "        # Rank shift for common items\n",
    "        franks = {it: r for it, r in zip(f, recs[p][\"full\"][\"ranks\"])}\n",
    "        vranks = {it: r for it, r in zip(v, recs[p][\"vendor\"][\"ranks\"])}\n",
    "        common = list(sf & sv)\n",
    "        if common:\n",
    "            shifts = [abs(franks[it] - vranks[it]) for it in common]\n",
    "            mean_shift = float(np.mean(shifts))\n",
    "            median_shift = float(np.median(shifts))\n",
    "        else:\n",
    "            mean_shift = np.nan\n",
    "            median_shift = np.nan\n",
    "\n",
    "        # Items only in each\n",
    "        only_full = [it for it in recs[p][\"full\"][\"items\"] if it.lower() not in sv]\n",
    "        only_vendor = [it for it in recs[p][\"vendor\"][\"items\"] if it.lower() not in sf]\n",
    "\n",
    "        pair_rows.append({\n",
    "            \"date\": p[0], \"mask_id\": p[1],\n",
    "            \"overlap_at_k\": overlap,\n",
    "            \"mean_rank_shift\": mean_shift,\n",
    "            \"median_rank_shift\": median_shift,\n",
    "            \"items_only_full\": only_full,\n",
    "            \"items_only_vendor\": only_vendor,\n",
    "        })\n",
    "\n",
    "    exact_match = pd.DataFrame({\n",
    "        \"metric\": [\"strict_exact_match_count\", \"strict_exact_match_rate\",\n",
    "                   \"set_exact_match_count\", \"set_exact_match_rate\"],\n",
    "        \"value\": [\n",
    "            strict_matches,\n",
    "            (strict_matches / len(inter_pairs)) if len(inter_pairs) else np.nan,\n",
    "            set_matches,\n",
    "            (set_matches / len(inter_pairs)) if len(inter_pairs) else np.nan,\n",
    "        ],\n",
    "    })\n",
    "\n",
    "    # -------------------------\n",
    "    # D) Confidence Comparison\n",
    "    # -------------------------\n",
    "\n",
    "    # Overall by track (SeriesGroupBy -> wide, then rename)\n",
    "    conf_overall = (\n",
    "        df.groupby(\"track\")[\"confidence\"]\n",
    "          .agg([\"mean\", \"median\", \"count\"])\n",
    "          .reset_index()\n",
    "          .rename(columns={\"mean\": \"mean_conf\", \"median\": \"median_conf\", \"count\": \"n\"})\n",
    "    )\n",
    "\n",
    "    # By rank position\n",
    "    conf_by_rank = (\n",
    "        df.groupby([\"track\", \"rank\"])[\"confidence\"]\n",
    "          .agg([\"mean\", \"median\", \"count\"])\n",
    "          .reset_index()\n",
    "          .rename(columns={\"mean\": \"mean_conf\", \"median\": \"median_conf\", \"count\": \"n\"})\n",
    "          .sort_values([\"rank\", \"track\"])\n",
    "    )\n",
    "\n",
    "    # Δconfidence for matched positions (pairs that have both tracks and that rank)\n",
    "    deltas = []\n",
    "    for p in inter_pairs:\n",
    "        f_map = {r: c for r, c in zip(recs[p][\"full\"][\"ranks\"],   recs[p][\"full\"][\"conf\"])}\n",
    "        v_map = {r: c for r, c in zip(recs[p][\"vendor\"][\"ranks\"], recs[p][\"vendor\"][\"conf\"])}\n",
    "        for r in range(1, TOP_K + 1):\n",
    "            if (r in f_map) and (r in v_map):\n",
    "                cf, cv = f_map[r], v_map[r]\n",
    "                if pd.notna(cf) and pd.notna(cv):\n",
    "                    deltas.append({\"rank\": r, \"delta_conf\": cf - cv})\n",
    "\n",
    "    if deltas:\n",
    "        deltas_df = pd.DataFrame(deltas)\n",
    "        conf_deltas_by_rank = (\n",
    "            deltas_df.groupby(\"rank\")[\"delta_conf\"]\n",
    "                     .agg([\"mean\", \"median\", \"count\"])\n",
    "                     .reset_index()\n",
    "                     .rename(columns={\"mean\": \"mean_delta\", \"median\": \"median_delta\", \"count\": \"n_pairs\"})\n",
    "                     .sort_values(\"rank\")\n",
    "        )\n",
    "    else:\n",
    "        conf_deltas_by_rank = pd.DataFrame(columns=[\"rank\", \"mean_delta\", \"median_delta\", \"n_pairs\"])\n",
    "\n",
    "    \n",
    "    # -------------------------\n",
    "    # E) Discrepancy Report (Top 20)\n",
    "    # -------------------------\n",
    "    discrepancies = pd.DataFrame(pair_rows)\n",
    "    if not discrepancies.empty:\n",
    "        discrepancies = discrepancies.sort_values(\n",
    "            by=[\"overlap_at_k\", \"mean_rank_shift\"],\n",
    "            ascending=[True, False]\n",
    "        ).head(20)\n",
    "\n",
    "    # -------------------------\n",
    "    # Plain-English Summary\n",
    "    # -------------------------\n",
    "    def bullet(text): return f\"• {text}\"\n",
    "    bullets = []\n",
    "    if len(inter_pairs):\n",
    "        bullets.append(bullet(\n",
    "            f\"Strict exact match on {strict_matches}/{len(inter_pairs)} pairs \"\n",
    "            f\"({(strict_matches/len(inter_pairs)):.1%}); set-exact match {(set_matches/len(inter_pairs)):.1%}.\"\n",
    "        ))\n",
    "    if pair_rows:\n",
    "        mean_overlap = np.nanmean([r[\"overlap_at_k\"] for r in pair_rows])\n",
    "        mean_shift   = np.nanmean([r[\"mean_rank_shift\"] for r in pair_rows])\n",
    "        bullets.append(bullet(\n",
    "            f\"Average overlap@{TOP_K}: {mean_overlap:.2f}; average rank shift for common items: {mean_shift:.2f}.\"\n",
    "        ))\n",
    "    if len(union_pairs):\n",
    "        bullets.append(bullet(\n",
    "            f\"Intersection rate (pairs with both lists): {(len(inter_pairs)/len(union_pairs)):.1%}.\"\n",
    "        ))\n",
    "    if not conf_deltas_by_rank.empty:\n",
    "        row1 = conf_deltas_by_rank[conf_deltas_by_rank[\"rank\"] == 1]\n",
    "        if not row1.empty:\n",
    "            bullets.append(bullet(\n",
    "                f\"At rank 1, mean Δconfidence (full−vendor) = {row1['mean_delta'].iloc[0]:.4f}.\"\n",
    "            ))\n",
    "\n",
    "    summary_text = \"\\n\".join(bullets) if bullets else \"• No intersection pairs to compare.\"\n",
    "\n",
    "    # -------------------------\n",
    "    # Return everything\n",
    "    # -------------------------\n",
    "    return {\n",
    "        \"coverage_volume\": coverage_volume,\n",
    "        \"exact_match\": exact_match,\n",
    "        \"conf_overall\": conf_overall,\n",
    "        \"conf_by_rank\": conf_by_rank,\n",
    "        \"conf_deltas_by_rank\": conf_deltas_by_rank,\n",
    "        \"discrepancies\": discrepancies,\n",
    "        \"summary_text\": summary_text,\n",
    "    }\n",
    "\n",
    "\n",
    "results = analyze_out(out, TOP_K=3)\n",
    "# print(results[\"coverage_volume\"])\n",
    "# print(results[\"exact_match\"])\n",
    "# print(results[\"conf_overall\"])\n",
    "# print(results[\"conf_by_rank\"])\n",
    "# print(results[\"conf_deltas_by_rank\"])\n",
    "# print(results[\"discrepancies\"])\n",
    "# print(results[\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e045cd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artifacts/model.joblib']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\"artifacts\").mkdir(exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "MODEL_PATH = \"artifacts/model.joblib\"\n",
    "joblib.dump(model, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d20870-5d69-4590-a79c-5b8640e623ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
